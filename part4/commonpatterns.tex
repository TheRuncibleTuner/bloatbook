\begin{comment}
chapter 1: basics of lifetime patterns
chapter 2: aside for basics of memory management
chapter 3: time-space tradeoffs (optimization)
chapter 4: basics+optimizations -> requiremnts; how to implement these things
chapter 5: out of the box




feasibly recomputable state
for recomputable, either recompute every time, recompute some times, recompute
never

sharing pool; flyweight; shared immutable factory pattern (coding pattern)

inmemory designs; time-space tradeoffs; correlated lifetime

three risks: introduce leaks, create concurrency problems, extra space
consumption

\end{comment}

\chapter{Understanding Your Lifetime Requirements}
\index{Lifetime Requirements}

Your application needs some objects to live forever and it needs the rest to die
a timely death. Unfortunately, some of the important details governing memory
management are left in your hands. Java promised, with its automatic memory
management, that you could create objects without regard for the messy details of
storage allocation and reclamation. In Java, you needn't explicitly free objects,
which is at once the saviour from, and the source of, many problems with memory
consumption. Unless you are careful, your program will suffer from bugs such as
memory leaks\index{Memory Leaks}, race conditions, lock contention, or excessive
peak footprint. Furthermore, if your objects don't easily fit into the limits of
a single Java process, you will need to manage, explicitly, marshalling them in
and out of the Java heap.\index{Marshalling}

Very often, your application uses a data structure in a way that falls into one
of a handful of common \emph{lifetime requirements}\index{Lifetime Requirements}. The
nature of each requirement dictates how much help you will get from the Java runtime
in the desired preservation and reclamaion of objects, and where it leaves you to
your own devices. 

An important step in the design process of any large application is understanding
the lifetime requirement for each of your data models. In this chapter, we
describe the five common lifetime requirements: objects needed only transiently,
objects needed for the duration of the run, objects whose lifetime ends along
with a method invocation, objects whose lifetime is tied to some other object,
and, most difficult of all, objects that live or die based on need.
\autoref{tab:five-lifetimes} summarizes these five important requirements. We step
you through each of the requirements, defining them and giving examples of how to
know when you have an instance of each.

\begin{table}
\centering
%	\begin{tabular}{lp{0.30\textwidth}p{0.35\textwidth}}
	\begin{tabular}{ll}
	\toprule
	  %&
	   Lifetime Requirement & Example \\ \cmidrule(r){1-1} \cmidrule(l){2-2}
	   %\cmidrule(r){2-2} \cmidrule(l){3-3}
	%\autoref{temporary-lifetime} &
	  Temporary & new parser for every date
	\\
	%\autoref{correlated-lifetime} &
	Correlated with Another Object & object annotations
	\\
	%\autoref{correlated-lifetime} &
	Correlated with a Phase or Request & tables needed only for parsing
	\\
	%\autoref{deferred-deletion} &
	Request/phase-spanning & session state 
	\\
	%\autoref{forever-lifetime} &
	 Permanently Resident & product catalog
	\\
%	\autoref{correlated-lifetime-2} & {Correlated with Phase} &
%	DOM used only for parsing
%	\\
	%\autoref{deferred-deletion} &
	 Time-space Tradeoff & database connection pool
	  \\
%period\\ scoped to a phase/request\\
%correlated with an object (annotations)\\
%correlated with need}\\ \hline
%reusable & maybe i'll need it later \\ \hline
	\bottomrule
	\end{tabular}
	\caption{Common requirements for the lifetime of objects that your
	application must implement properly, in order to avoid correctness or
	performance problems.}
	\label{tab:five-lifetimes}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{part4/Figures/requirements}
\caption{Illustration of some common lifetime requirements.}
\label{fig:five-lifetimes}
\end{figure}

Once you have mapped out the lifetime requirements of your data models, the next
step is to chose the right implementation details in order to correctly implement
each requirement. The remaining chapters in this part show how to implement these
requirements.

%The trickier aspects of memory management, summarized in
%\autoref{tab:tricky-memory-management}, are discussed in greater detail in
%later chapters.


\section{Object Lifetimes in A Web Application Server}

%Configuring memory settings is an iterative process. It usually involves a
%fair amount of trial and error, as one tunes the various knobs to balance memory
%consumption and application performance. These knobs affect things like the size of the Java
%heap, how many entries a cache should hold, and the timeout value for these
%caches. This is usually a process of black box tuning: twist a knob, and see
%how overall performance changes. In addition to being hit
%and miss, it is also quite prone to bugs. If you set the size of a cache too
%high, you risk poor performance due to excessive garbage collection, and even
%possibly process failures, due to running out of heap space.

To introduce the common requirements for object lifetime, we walk through several
scenarios found in most long-running server applications. These applications
provide an interesting case study for lifetime management. Managing lifetime when
the application runs forever is an especially complex issue. This is true for
more than for servers alone. Desktop applications such as the Eclipse integrated
development environment shares many of the same challenges. Improperly managing
the lifetimes of objects, for short-running applications, often does not result
in critical failure. Indeed, the application often finishes its run before one
would even notice a problem with memory consumption. Plus, you're probably don't
run many instances of a short-running application simultaneously; and so
achieving the ultimate in scalability is not a primary concern. In contrast, if
an application runs more or less forever, then mistakes pile up over time. In
addition, caching plays a large role in these applications, since they often
depend on data fetched from remote servers, or from disk, neither of which can
support the necessary throughput and response time requirements. The ability for
mistakes to pile up, and for misconfigured or poorly implemented caches to impede
performance means that special care must be taken when implementing your server application.

The heap consumption of this application will fluctuate over time. A timeline
view of expected memory consumption helps to visualize these changes. It
visualizes memory during the lulls and peaks of activity, as requests are
processed and when sessions time out, and as the server starts up. We will use
these over the next few pages, as we walk through the common cases of lifetime
requirements.




% \begin{example}{Lifetime in A Web Application Server}
To help introduce the common lifetime requirements, we walk through an example of
a shopping cart server application. The server, on startup, preloads catalog data
into memory to allow for quick access to this commonly used data. It also
maintains data for users as they interact with the system, browsing and buying
products. Finally, it caches the response data that comes from a remote service
provider that charges per request. Let's walk through how we can plan out the
lifetime requirements of these data structures.
% How does Java heap consumption vary over time? Which heap size fluctuations
% indicate a problem, and which are expected behavior? \end{example}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{part4/Figures/lifetime/timeline-base-session-temps}
	\caption{Memory consumption, over time, typical of a web application server.}
	\label{fig:timeline-base-session-temps}
\end{figure}

\paragraph{Lifetime Requirement: Permanently Resident}
\autoref{fig:timeline-base-session-temps} shows the timeline of memory
consumption of our example server 
during and shortly after its initial startup.
During the startup interval, the server preloads catalog data into
the Java heap. Then, the server is warmed with with two test requests.
The total height of the area
under the curves represents the memory consumption at that point in time. 
The preloaded catalog
data will be used for the entire duration of the server process.
\index{Objects That Live Forever}
Therefore, the Java objects that represent this catalog are objects that are
needed forever. In the timeline picture, this data is respresented by the lowest area, labeled
\emph{baseline}. Notice how it ramps up quickly, and then, after the server has
reached a ``warmed up'' state, memory consumption of this baseline data evens out
on a plateau for the remainder of the run.

\paragraph{Lifetime Requirement: Request-spanning}
\index{Session State}
After the server is warmed up, it begins to process client requests. Imagine
interacting with a commerce site with a web browser. First you browse around,
looking for items that you like, and add them to your shopping cart. Eventually,
you may authenticate and complete a purchase. As you browse and buy, the server
maintains some state, to remember aspects of what you have done so far. For
example, the server stores the incremental state of multi-step transactions,
those that span multiple page views.
 This session
state, at least the part of it stored in the Java heap, will go away soon after
your browsing session is complete. In the timeline figure, this portion of memory
is labeled \emph{sessions}. It ramps up while a session is in progress, and then,
in the example illustrated here, soon all of that session memory should be
reclaimed.

\paragraph{Lifetime Requirement: Temporaries}
The catalog (baseline) data and session state are both examples of objects that
are expected to stick around for a while.
\index{Temporary Objects}
 In the course of preloading the cache
and responding to client requests, the server application will create a number
of objects that are only used for a very short period of time. They help to
faciliate the main operations of the server.
These temporary objects will be reclaimed by the \jres garbage collector in
relatively short order. The point at which an object is reclaimed depends on when the garbage collector
notices that it is reclaimable. Normally, the garbage collector will wait until
the heap is full, and then inspect the heap for the objects that are still
possibly in use. In this way, the area under the \emph{temporaries} curve has a
see-saw shape. As the temporaries pile up, waiting for the next garbage
collection, they contribute more and more to memory footprint. Normally, once
the \jre runs a garbage collection, these temporaries no longer in use will no
longer contribute to heap consumption.

In this way, temporary objects
\emph{fill up the headroom} in the heap.\index{Heap Headroom} If there is a
large amount of heap space unused by the longer-lived objects, then the
temporaries can be reclaimed less often. This is a good thing, because a
garbage collection is an expensive proposition.
\index{Heap Size Settings} \index{Maximum Heap Size} \index{-Xmx}
When configuring your application, you may specify a maximum heap size. It
should certainly be larger than the baseline and session data. How much
larger than that? This choice directly affects the amount of \emph{headroom},
 that is the amount of space available for temporaries to pile up.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{part4/Figures/lifetime/timeline-base-session-temps-with-leak}
	\caption{If session state is not cleaned up
	properly, a memory leak is the result. This means more frequent garbage
	collections, and ever increasing heap size.}
	\label{fig:timeline-base-session-temps-with-leak}
\end{figure}

\paragraph{Lifetime Requirement: Correlated Lifetime}
\index{Memory Leaks}
The catalog data should last forever, while the
session data lives for some bounded period of time. It is possible that session
state will live beyond the end of your session, but nonetheless it has a
lifetime that is bounded. If, due to an bug, part of this session state is not
reclaimed, the application will leak memory. Though it is supposed to have a bounded lifetime, it
%\marginpar{\textbf{Memory Leaks} are still possible, even with
%automatic garbage collection!} 
accidentally lives forever. In this
case, over time, the amount of heap required for the application to run will increase without bound.
\autoref{fig:timeline-base-session-temps-with-leak} illustrates this situation,
in the extreme case when all of session state leaks. Over time, the area under
the curve steps higher and higher.

%As you scan the timeline from left to right, memory consumption 
%it fetches catalog data from its database, and stores them in the Java heap. 

\paragraph{Lifetime Requirement: Time-space Tradeoffs}
\index{Time-Space Tradeoffs}
Finally, this example server caches data from some expensive third-party data
source. When caching data inside of Java objects, there is a fourth effect on
the timeline landscape. The
cached data must be configured properly to live long enough to be useful. It
also must not occupy so much of the heap so as to leave little headroom for
temporaries. \autoref{fig:timeline-base-session-temps-with-cache} shows an
example where the cache has probably been configured to occupy too much heap
space. Observe how, compared to the other timeline figures, there is little
headroom for temporaries. In this case, the result is more frequent garbage
collections. If the cache were sized to occupy an even greater amount of heap
space, it is possible that there would no longer be room to fit session data.
The result in this case would be failures in client requests. 
Sizing caches is important, but tricky to get right.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{part4/Figures/lifetime/timeline-base-session-temps-with-cache}
	\caption{When a cache is in use, this leaves less headroom for temporary
	object allocation, often resulting in more frequent garbage collections.}
	\label{fig:timeline-base-session-temps-with-cache}
\end{figure}

% introduced by example in this chapter, and
%Many objects are either temporaries or needed for
%the entire run of your application. Sometimes you create objects whose lifetime
%is correlated with other objects or that should go away when a method
%invocation completes. Sometimes you need to manage objects hanging around
%longer than their current need, to avoid future recomputations or refetching
%of data in the case when it is needed in the near future. 

%\begin{table}
%\centering
	%\begin{tabular}{ll} \toprule
    %%%%	%& Things Java Doesn't Do Automatically \\ \cmidrule{2-2}
%    	\autoref{avoiding-lifetime-bugs} & {Avoiding Memory Leaks} \\
%    	\autoref{balance-time-and-space} & {Balancing Time and Space} \\
%    	\autoref{outisde-java-box} & {Supporting Massive Data Sets}  	\\
%        \bottomrule
%    \end{tabular}
%	\caption{The tricky aspects of memory management.}
%	\label{tab:tricky-memory-management}
%\end{table}

\section{Temporary Objects}
\label{temporary-lifetime}

If your application is like most Java applications, it creates a large number of
temporary objects. They hold data that will only be used for a very short
interval of time. It is often the case that the objects in these transient data
structures are only ever reachable by local variables. For example, this is the
case when you populate a \class{StringBuilder}, turn it into a \class{String},
and then ultimately (and only) print the string to a log file. The point at
which these objects, the string builder, string, and character arrays, are no
longer used is only shortly after they are constructed:

\begin{shortlisting}
String makeLogString(String message, Throwable exception) {
	StringBuilder sb = new StringBuilder();
	sb.append(message);
	sb.append(exception.getLocalizedMessage());
	return sb.toString();
}
void log(String message, Throwable exception) {
	System.err.println(makeLogString(message, exception));
}
\end{shortlisting}

A temporary object serves as a transient home for your data, as it makes its way
through the frameworks and libraries you depend on. Temporaries are often
necessary to bridge separately developed code and enable code reuse. The above
example avoids code dupliation and ensures uniformity of the output data by
factoring out the logic of formatting messages into the \code{makeLogString}
method.

%By converting your data layout into a form that an API requires,
%then you can reuse the functionality it provides.

In many cases, the \jre will do a sufficient job in managing these temporary
objects for you. Generational garbage collectors \index{Generational Garbage
Collection} these days do a very good job digesting a large volume of temporary
objects. In a generational garbage collector, the \jre places temporary objects
in a separate heap, and thus need only process the newly created objects, rather
than all objects, during its routine scan.

There are two potential problems that you may encounter with temporary objects.
The first is the runtime cost of initializing the state of the temporary
objects' fields. Even if allocating an freeing up the memory for an object is
free, there remains the work done in the constructor:

\begin{shortlisting}
class Temp {
	private final Date date;
	
	public Temp(String input) {
		this.date = DateFormat.getInstance().parse(input);
	}
}
\end{shortlisting}

Even if an instance of \class{Temp} lives for only a very short time, its
construction has a high cost. It is often the case that this expense is hidden
behind a wall of APIs. If so, then what you think of as trivial temporary (since
you, after all, are in control of when the instance of \class{Temp} lives and
dies), would in actuality be far from trivial in runtime expense. Expenses can
pile up even further if temporary object constructions are nested.

There is a second potential problem with temporary objects. By creating temporary
objects at a very high rate, it is possible to overwhelm either the garbage
collector, or the physical limitations of your hardware. For example, at some
point, the memory bandwidth necessary to initialize the temporary objects will
exceed that provided by the hardware. Say your application fills up the temporary
heap ever second. In this case, based on the common speeds of garbage collectors,
your application could easily
 spend over 20\% of its time collecting garbage. Is it difficult to fill up the
 temporary heap once per second? Typical temporary heap sizes run around 128
 megabytes. Say your application is a serves a peak of 1000 requests per second,
 and creates objects of around 50 bytes each. If it creates around 2500
 temporaries per request, then this application will spend 20\% of its time
 collecting garbage.


%A great many of these
%temporary structures serve the role of a kind of lubricant, making it easy for
%you to write code that ties together the separately written parts of your code
%base and reuses standard libraries as much as possible. Often, these are
%objects that are not a fundamental necessity of what you're trying to
%accomplish. If
%you had the freedom to code highly specialized implementations of the important
%cases, from scratch, many of these temporary structures would be unnecessary.

\begin{example}{How Easy it is to Create Lots of Temporary Objects}
A common example of temporaries is parsing
and manipulating data coming from the outside world. Identify the temporary
objects in the following code.
% to the wire?

\begin{shortlisting}%[float,caption=Code that constructs 8 temporary objects tohandle two dates.,label=TempExampleCode]
void main(String xy) {
	doWork(xy.substring(0,10), xy.substring(10));
}	
void doWork(String x, String y) {
	doRemoteProcedureCall(parse(x));
	doRemoteProcedureCall(parse(y));
}
Date parse(String string) {
	return new SimpleDataFormat().parse(string, new ParsePosition(0));
}
void doRemoteProcedureCall(Date date) {
	long timestamp = date.getTime();
	...
}
\end{shortlisting}
\end{example} 

This code starts in the \code{main} method by splitting the input string into two
substrings. So far, the code has created four objects (one \class{String} and one
character array per substring). Creating these substrings makes it easy to use
the \code{doWork} method, which takes two Strings as input. However, observe that
these four objects are not a necessary part of the computation. Indeed, these
substrings are eventually used only as input to the \class{SimpleDateFormat}
\code{parse} method, which has been nicely designed to allow you to avoid this
very problem. By passing a \class{ParsePosition}, one can parse substrings of a
string without having to create temporary strings (at the expense of creating
temporary \class{ParsePosition} objects!).



% correlated with need: as soon as last user goes away, remove his stuff 
% share common expressions to save space, but using strong references -> memory
% leak; plugins in eclipse go away when all views
% sharing pool 

% weak ref keys -> annotation
% weak ref values -> sharing pool

% soft ref values -> caching

% annotation by map lookup


\section{Permanently Resident}
\label{forever-lifetime}

A \class{SimpleDateFormat} object in the previous example was created in every
loop iteration, and never used again. An improvement would be to create and
use a single formatter for the remaining duration of the run. Though the Java 6
documentation does not say so explicitly, it is safe to reuse a single instance
of this object multiple times. You must be careful to remember that it is
not safe to do so in multiple threads. The next chapter will discuss remedies
to this problem. The updated code for the \code{parse} method would be:

\begin{shortlisting}
static final DateFormat fmt = new SimpleDateFormat();

Date parse(String string) {
	return fmt.parse(string, new ParsePosition(0));
}
\end{shortlisting} 

\section{Objects with Correlated Lifetimes}
\label{correlated-lifetime}

Many objects are needed for a very specific interval of time. This interval is
usually defined either by the lifetime of another object, or by the duration of
a method call. Once that other object is not needed, or once that method
returns, then these objects are also no longer needed. These are the two many
cases of objects with correlated lifetime.

\subsection{Objects that Live and Die Together}
\label{correlated-lifetime-1}

Normally, if you need to augment the state stored an object, you modify
the source code of some existing classes. For example, to add a secondary
mailing address to a \class{Person} object, you can add a field to that class, and update the
initialization and marshalling logic accordingly. Sometimes, you will find it
necessary to associate information with an object that is, for one reason or the
other, locked down.

\begin{example}{Annotations}
In order to debug a performance problem, you need to associate a timestamp with
another object. Unfortunately, you don't have access to the source code for
that object's class. Where do you keep the new information, and how can you
link the associated storage to the main objects without introducing memory
leaks?\index{Memory Leaks}
\end{example}

If you can't modify the class definition for that object, then you will have to
store the extra information elsewhere. These \emph{side annotations}\index{Side
Annotations} will be objects themselves, and you need to make sure that their
lifetimes are correlated with the main objects. When one dies, the other
should, too.

You could store the annotations in a map that is keyed by the
original object, say of type \class{T}:

\begin{shortlisting}
Map<T, Date> timestamps = new HashMap<T, Date>();

void addTimestamp(T t) {
	timestamps.put(t, new Date());
}
Date getTimestamp(T t) {
	return timestamps.get(t);
}
\end{shortlisting}

This solution will function correctly, but suffers from a \emph{memory
leak}\index{Memory Leak}. As the application runs, it will consume greater
amounts of Java heap, up until the point when the \jre runs out of heap space to
allocate any more objects. This solution leaks memory, because the
\code{timestamps} map introduces a reference to the main objects. When the
garbage collector scans the heap to see which objects are still alive, the
references in this map will be among those that keep the objects alive. The next
chapter discusses these issues in more detail. An improved solution would use the
\class{WeakHashMap} from the Java standard libraries. By replacing the
initialization of the \code{timestamps} map, we have the same functionality as
before, but no memory leak.

\begin{shortlisting}
Map<T, Date> timestamps = new WeakHashMap<T, Date>();
\end{shortlisting}

Note that this same situation can hold even if you are able to modify the class
definition. A common scenario requires annotations on only a subset of all
instances of a class. In this case, is it not worth paying the memory cost to
have the ability to annotate every single instance. Therefore, this is another
case where a solution of side annotations, stored in a \class{WeakHashMap},
shines.

\subsection{Objects that Live and Die with Program Phases}
\label{correlated-lifetime-2}

Similar to the way the lifetime of an object can be correlated with another
object, lifetimes are often correlated with method invocations. When a method
returns, objects correlated with it should go away. For temporary objects, this
is usually easy to ensure, since they are usually only reachable from stack
locations. For the medium-to-long running methods that implement the
core functionalities of the program, this correlation is harder to get right.

For example, if your application loads a log file from disk,
parses it, and then displays the results to the user, it has roughly three
phases for this activity. Most of the objects allocated in one phase are scoped to that
phase; they are needed to implement the logic of that phase, but not subsequent
phases. The phase that loads the log file is likely to maintain maps that
help to cross reference different parts of the log file. These are necessary
to facilitate parsing, but, once the log file has been loaded, these maps can be
discarded. In this way, these maps live and die with the first phase of this
example program. If they don't, because the machinery you have set up to
govern their lifetimes has bugs, then your application has a memory
leak\index{Memory Leaks}.

This lifetime scenario is also common if your application is an
server that handles web requests.

\begin{example}{Memory Leaks in an Application Server}
	A web application server handles servlet requests. How is it possible that
	objects allocated in one request would unintentionally survive beyond the end
	of the request?
\end{example} 
  
In server applications, most
objects created within the scope of a request should not survive the
request. Most of these \emph{request-scoped}
\index{Request-scoped Lifetime} objects are not used by the application after the
request has completed. In the absence of application or framework bugs, they will
be collected as soon as is convenient for the runtime. In this example, the
lifetime of objects during a request are \emph{correlated} with a method
invocation: when the servlet \class{doGet} or \class{doPut} (etc.) invocations
return, those correlated objects had better be garbage collectible.

\index{Memory Leaks: Why?}
There are many program bugs and configuration missteps that can lead to
problems. The general problem is that a reference to an object stays around
indefinitely, but becomes
\emph{forgotten}, and hence rendered unfindable by the normal application
logic. If this request-scoped data structure were only reachable from stack
locations, you would be fine. Therefore, a request-scoped object will leak only
when there exist references from some data structure that lives forever. Here
are some common ways that this happens.

\begin{itemize}
  \item Registrars, where objects are registered as listeners to some service,
  but not deregistered at the end of a request.
  \item Doubly-indexed registrars. Here the outer map provides a key to index
  into the inner map. A leak occurs when the outer key is mistakenly
  overwritten mid-request. This can happen if the namespace of keys isn't
  canonical and two development groups use keys that collide. It can also
  happen if there is a mistaken notion, between two development groups, of who
  owns respnsibility of populating this registrar.
  \item Misimplemented hashcode or equals, which foils the retrieval of an
  object from a hash-based collection. If developers checked the return value of
  the \code{remove} method, which for the standard collections would indicate a failure to remove, then
  this bug could be easily detected early; but developers tend not to do this.
\end{itemize}

The next chapter goes into greater detail on how to avoid these kinds of
errors. \autoref{chapter:tools} describes tooling that can help you
detect and fix the bugs that make it into your finished application.


%\subsection{Correlated with Need} % do we need this? isn't session state a
% deferred deletion policy?
%\label{correlated-lifetime-3}

\section{Object Uses that Span Requests}
\label{deferred-deletion}

The last important facet of object lifetime comes from those objects that stay
around beyond the scope of any one method or object. These objects must survive
for some indeterminant amount of time. In some cases, this period based on the
profitability of keeping them around. In other cases, objects need to be kept
around for operations that span several independent operations across multiple
threads.
There are three important cases of objects that need to be reclaimed in some
deferred fashion: caches, sharing pools, and resource pools.

\subsection{Caches: Buying Time with Space}
\label{sec:caches}
\index{Caches}

\marginpar{A \textbf{Cache} is a map that holds expensive data values, each
accessed by a unique key.}
If the data stored in an object is cheap to recompute or refetch from an
external data source, then a good policy would be to treat the object as a
temporary. Performing a few dozen machine instructions is very likely to be a
worthwhile tradeoff, if memory consumption is the limiting resource for
scaling up\index{Scaling Up}. 
You do have to be careful, though. Recall our earlier example that creates a new
instance of the date parser \class{SimpleDateFormat} for each iteration of a
loop. Here, as is quite common, something expensive to compute, that
\class{SimpleDateFormat}, is treated as a temporary. If the data stored in an
object is expensive to recompute or refetch, and there is some chance it might
be used in the future, then it is worthwhile to keep it around.

\index{Time-Space Tradeoffs}
Finding the right balance of time and space is the goal of a good cache
implementation. The expense of re-fetching data from external data sources and
recomputing the in-memory structure can often be amortized, at the expense of
stretching the lifetime of these data structures. By increasing the actual
lifetime on an object you will very likely increase peak memory consumption. A
good cache defers the time that an object will be reclaimed, as long as there is
sufficient space to handle the flux of temporary objects your application
creates. It holds on to a data structure after the current operation is finished
with it, in the hope that other operations in the near future will reuse it.

\subsection{Sharing Pools: Avoiding Data Replication}
\label{sec:sharing-pools}
\index{Sharing Pool}

\marginpar{A \textbf{Sharing Pool} stores canonical instances of data
values that would otherwise be replicated in many objects.}
A cache amortizes the time cost of fetching or initializing data. An orthogonal
issue lies in the memory expense of storing many copies of the same data
throughout the heap. This is especially a problem with strings. Heaps can often
store the same string a dozen times.

\begin{example}{Duplicate Strings}
You application loads data from a file. This data contains a large number of
name-value maps that will be used frequently throughout program execution.
These maps represent configuration information. The names come from a small set
of 16 distinct names. The values are strings come from
a set of strings unknown at development time, but a set that is small in size;
there aren't going to be many distinct values, but you are unwilling or unable
to nail them down at compile time. How can these maps be stored in a memory
efficient way?
\end{example}

Without any special effort, each instance of this kind of configuration map
would store the some subset of same 16 key strings. Furthermore, each map would
store duplicates of the values. The following code snippet has those two
aspects of duplication:

\begin{shortlisting}
void handleNextEntry() {
	String key = getNextString();
	Object value = getNextString();
	map.put(key, value);
}
\end{shortlisting} 

Java provides a built-in mechanism for sharing the contents of strings across
many string instances. By \emph{interning}\index{String interning} a Java
\class{String}, you ensure that the returned \class{String} will only have
distinct storage if it is a string value that hasn't been interned yet. You can
modify the first try as follows:

\begin{shortlisting}
void handleNextEntry() {
	String key = getNextString().intern();
	Object value = getNextString().intern();
	map.put(key, value);
}
\end{shortlisting} 

It is possible to do even better, if you have the luxury of modifying both ends
of the communication channel, i.e. both the serialization and this
deserialization code. There are only 16 distinct names used in all instances of
this configuration map. This seems like a perfect case for an enumerated type.
An enumerated type can be used to represent strings as numbers at runtime. The
only place the strings are stored is in the string constant pool\index{Java's
Constant Pools}. Each class, when compiled, keeps a pool of the strings that
are used by code in that class. In this way, an enumerated type is an even more
highly optimized sharing pool than that provided by the interning mechanism:

\begin{shortlisting}
enum PropertyName = {...};
void handleNextEntry() {
	PropertyName key = getNextPropertyName();
	Object value = getNextString().intern();
	map.put(key, value);
}
\end{shortlisting} 



There is an important variant of a sharing pool called the Bulk Sharing Pool.
Like a normal sharing pool, the goal of a bulk sharing pool is to amortize the
memor costs of storing data. However, rather than mitigate the costs of data
duplication, a bulk sharing pool aims to amortize the costs of Java object
headers across the elements in a pool. This is a topic that stretches notions of
how to store data beyond the normal Java box, and so will be discussed, along
with many similar matters, in \autoref{chapter:outisde-java-box}.


\begin{table}
	\centering
	\begin{tabular}{rlll} \toprule
            & cache             & sharing pool & resource pool 
    \\ \cmidrule{2-4}
%    What is Saved & alloc. and data init. time & duplicates &
    %alloc. time 
%    \\
    Addressing the Contents     & by key       & by index & by key
    \\
        Elements Interchangeable?   & no    & no    & yes
    \\
    Multiple Users per Element? & yes   & yes   & no
    \\
    Data Persists Across Uses?  & yes   & yes   & no
    \\ \bottomrule
    \end{tabular}
	\caption{Comparing the charaacteristics of three mechanisms for keeping data
	or objects around for indefinite periods of time.}
	\label{tab:three-deferred-deletions}
\end{table}

\subsection{Resource Pools: Amortizing Allocation Costs}
\label{sec:resource-pools}
\index{Resource Pool}

\index{Amortizing Costs}
A cache can amortize the cost, in time, of fetching or otherwise initializing the
data stored in an object. A sharing pool can amortize the cost, in space, of
storing the same data in many separate objects. In both cases, the data is the
important part of what is stored.

\marginpar{A \textbf{Resource Pool} is a set of interchangeable storage or
external connections that are expensive to construct.} There is a third case,
where one needs to amortize the cost of the allocations, rather than the cost of
initializing or fetching the data that is stored in this object. A resource pool
stores the result of the allocation, not the data. Therefore, the elements of a
resource pool are interchangeable, because it is the storage, not the values that
matter. It is important to note that, though the data values are not the
important part, the elements of the pool are objects, and are thus intended to
store data! A resource pool handles the interesting case where the data is
temporary, but you need, for performance reasons, the objects to live across many
uses. The protocol for using a resource pool then involves reservation, a period
of private use of the fields of the reserved object, followed by a return of that
object to the pool.

Resource pooling only makes sense if the allocations themselves are expensive.
There are several reasons why a Java object can be expensive to allocate.
Creating and zeroing a large array\index{Large Arrays} in each iteration of a
loop can bog down performance. Creating a new key object to determine whether an
value exists in a map can sometimes contribute a great deal to the load of
temporary objects.

\index{Connection Pools}
A more important example of the need for amortizing the time cost of allocation
comes when this Java object is a proxy for resources outside of Java. If your
application accesses a relational database through the JDBC\index{JDBC}
interface, you will experience the need for resource pooling. There are two kinds
of objects that serve as proxies for resources involving database access. First
are the connections to the database. In most operating systems, establishing a
network connection is an expensive proposition. It also involves reservation of
resoures in the database process. Second are the precompiled SQL statements that
your application uses. As with the connections, these involve setup cost, of the
compilation itself, as well as the reservation of memory resources, that the
database uses to cache certain information about the query.

\begin{example}{Per-thread Singleton versus Resource Pool}
\index{Thread-Local Storage}
Your application accesses a remote resource. Why not keep one connection
persistent per thread? What's the point of a resource pool in this case?
\end{example}


%% OLD STUFF NMM 20090820
%\section{Request Scoping}
%\section{Correlated Lifetime}
%\paragraph{Weak and Soft references in Java}
%\section{Memory Leaks and Drag}
%\section{Examples}
%\subsection{Transient Near-Copies}
%\subsubsection{String Canonicalization}
%\subsection{Temporary Collections}
%\subsection{Facilitators}
