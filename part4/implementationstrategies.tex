\chapter{Implementation Practices for Sound Lifetime Management}
\label{chapter:lifetime-implementation-strategies}

Typically, objects die soon after the point in time of their last use, once all
dominating references are removed or naturally go out of scope. For a great many
objects, the normal flow of method invocations results in local variables going
out of scope, which renders these objects reclaimable without any special effort
on your part. In the absence of memory leaks, and without any optimizations,
objects live and die according to this \emph{natural lifetime}, as discussed in
depth in \autoref{sec:natural-lifetime}. 

However, the built-in lifetime mechanisms, by relying on objects going out of
scope, are insufficient to implement the more complicated patterns.
Implementations of the correlated lifetime pattern, introduced in
\autoref{sec:correlated-lifetime-pattern}, are very prone to memory leaks. You
may need an object to survive for a period of time that is not bound to any one
method invocation, but rather to the lifetime of another object. The lifetime of
some objects are indeed correlated with an invocation, as in the case of objects
correlated with a phase or request, but even here there are difficulties.
Oftentimes, the invocation that marks the beginning of a request is in a part of
the code outside of your control, or is distant from the allocation site of the
objects that must go away when the request finishes. Implementations of the
time-space tradeoff pattern, introduced in
\autoref{sec:time-space-tradeoffs-pattern}, can be ineffective if they aren't
sized properly. They, too, can result in memory exhaustion, e.g. if a cache's key
misimplement equality, or if it is sized too large.

It is important to code according to practices that will assure that an object
dies when it should. The correlated lifetime and time-space tradeoff patterns are
the most difficult cases to get right, and so those most in need of rigorous
coding practices.

% managing a reference queue

\section{Safeguards}
\subsection{Single Strong Reference Principle}
\subsection{home base}

\section{}
\subsection{Annotations}

You could store the annotations in a map that is keyed by the
original object, say of type \class{T}:

\begin{shortlisting}
Map<T, Date> timestamps = new HashMap<T, Date>();

void addTimestamp(T t) {
	timestamps.put(t, new Date());
}
Date getTimestamp(T t) {
	return timestamps.get(t);
}
\end{shortlisting}

This solution will function correctly, but suffers from a \emph{memory
leak}\index{Memory Leak}. As the application runs, it will consume greater
amounts of Java heap, up until the point when the \jre runs out of heap space to
allocate any more objects. This solution leaks memory, because the
\code{timestamps} map introduces a reference to the main objects. When the
garbage collector scans the heap to see which objects are still alive, the
references in this map will be among those that keep the objects alive. The next
chapter discusses these issues in more detail. An improved solution would use the
\class{WeakHashMap} from the Java standard libraries. By replacing the
initialization of the \code{timestamps} map, we have the same functionality as
before, but no memory leak.

\begin{shortlisting}
Map<T, Date> timestamps = new WeakHashMap<T, Date>();
\end{shortlisting}

Note that this same situation can hold even if you are able to modify the class
definition. A common scenario requires annotations on only a subset of all
instances of a class. In this case, is it not worth paying the memory cost to
have the ability to annotate every single instance. Therefore, this is another
case where a solution of side annotations, stored in a \class{WeakHashMap},
shines.

\subsection{Annotation Pool}

\subsection{Listeners}

\callout{listener-rule}{Listener Rule}{
Listener queues should weakly reference the callback object. This obviates to
maintain and debug code that explicitly deregisters the callback hook from the
listener queue.}


\section{Safety Valves}

\callout{soft-reference-rule}{Soft Reference Rule}{
Soft references must always be over values, not keys. Otherwise, testing
equality of keys will trigger a use of the reference. This will extend the
lifetme of the value, even though the only use of the entry was in checking to
see if its key matches another.	}

\section{Concurrency Issues}
\label{sec:lifetime-management-concurrency-issues}

If your program operates with many concurrent threads, you have to program
differently, because straightforward implementations of the above strategies will
result in concurrency issues. One of the primary problems will be lock
contention, as threads concurrently poll a reference queue. An important example
of this problem shows up in the implementation of a cache that can support many
concurrent users.\index{Caches, Concurrency Issues}

A cache is a map, usually of bounded size, with an eviction strategy for
maintaining that bound.\index{Caches} The Java standard library provides a
concurrent map implementation, in the form of the \class{ConcurrentHashMap}
class, but this is not a cache, because it has no eviction hooks with which one
can bound its size.

Following the soft reference rule, and using the basic guidelines for managing
reference queues from \autoref{sec:reference-queue-basics}, leads to a first
attempt at a \class{ConcurrentCache} implementation. You can extend the basic
concurrent hashmap, wrapping the map's values with soft references:
\begin{shortlisting}
class ConcurrentCache<K,V> extends ConcurrentHashMap<K,SoftReference<V>> {
   private final ReferenceQueue<V> refQueue = new ReferenceQueue<V>();
   
   protected void cleanupQueue() {
      SoftReference<V> v;
      while( (v = refQueue.poll()) != null) {
         remove(???); // oops!
      }
   }
}
\end{shortlisting}
Oops! This implementation provides no way to remove the key from the map, when
cleaning up the evicted entries. To fix this, you'll need to stash a pointer to
the key in the soft reference wrapper. It would be nice if the
\class{ConcurrentHashMap} implementation let you extend
its implementation so that its \class{\$Entry} class extended soft reference;
the \class{\$Entry} would serve this role perfectly. Instead, you have to
replicate this pointer structure, at a silly but unavoidable cost of memory
bloat. You can do so in a \class{CacheSoftReference} wrapper:
\begin{shortlisting}
class CacheSoftReference<K, V> extends SoftReference<V> {
   private final K k;
   
   public CacheSoftReference(K k, V v, ReferenceQueue<V> refQueue) {
      super(v, refQueue);
      this.k = k;
   }
}

class ConcurrentCache<K,V> extends ConcurrentHashMap<K,CacheSoftReference<K,V>> {
   private final ReferenceQueue<V> refQueue = new ReferenceQueue<V>();
   
   protected void cleanupQueue() {
      SoftReference<V> v;
      while( (v = refQueue.poll()) != null) { // lock contention!
         remove(v.k);
      }
   }
   
   public void put(K key, V value) {
      cleanupQueue();
      super.put(key, new CacheSoftReference<K,V>(key,value,refQueue));
   }
   
   public V get(K key) {
      cleanupQueue();
      return super.get(key);
   }
}  
\end{shortlisting}
This implementation still suffers from many problems. First, 
