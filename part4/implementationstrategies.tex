\chapter{Implementation Practices for Sound Lifetime Management}
\label{chapter:lifetime-implementation-strategies}

Typically, objects die soon after the point in time of their last use, once all
dominating references are removed or naturally go out of scope. For a great many
objects, the normal flow of method invocations results in local variables going
out of scope, which renders these objects reclaimable without any special effort
on your part. In the absence of memory leaks, and without any optimizations,
objects live and die according to this \emph{natural lifetime}, as discussed in
depth in \autoref{sec:natural-lifetime}. 

However, the built-in lifetime mechanisms, by relying on objects going out of
scope, are insufficient to implement the more complicated patterns.
Implementations of the correlated lifetime pattern, introduced in
\autoref{sec:correlated-lifetime-pattern}, are very prone to memory leaks. You
may need an object to survive for a period of time that is not bound to any one
method invocation, but rather to the lifetime of another object. The lifetime of
some objects are indeed correlated with an invocation, as in the case of objects
correlated with a phase or request, but even here there are difficulties.
Oftentimes, the invocation that marks the beginning of a request is in a part of
the code outside of your control, or is distant from the allocation site of the
objects that must go away when the request finishes. Implementations of the
time-space tradeoff pattern, introduced in
\autoref{sec:time-space-tradeoffs-pattern}, can be ineffective if they aren't
sized properly. They, too, can result in memory exhaustion, e.g. if a cache's key
misimplement equality, or if it is sized too large.

It is important to code according to practices that will assure that an object
dies when it should. The correlated lifetime and time-space tradeoff patterns are
the most difficult cases to get right, and so those most in need of rigorous
coding practices.

% managing a reference queue

\section{Safeguards}
\subsection{Single Strong Reference Principle}
\subsection{home base}

\section{}
\subsection{Annotations}

You could store the annotations in a map that is keyed by the
original object, say of type \class{T}:

\begin{shortlisting}
Map<T, Date> timestamps = new HashMap<T, Date>();

void addTimestamp(T t) {
	timestamps.put(t, new Date());
}
Date getTimestamp(T t) {
	return timestamps.get(t);
}
\end{shortlisting}

This solution will function correctly, but suffers from a \emph{memory
leak}\index{Memory Leak}. As the application runs, it will consume greater
amounts of Java heap, up until the point when the \jre runs out of heap space to
allocate any more objects. This solution leaks memory, because the
\code{timestamps} map introduces a reference to the main objects. When the
garbage collector scans the heap to see which objects are still alive, the
references in this map will be among those that keep the objects alive. The next
chapter discusses these issues in more detail. An improved solution would use the
\class{WeakHashMap} from the Java standard libraries. By replacing the
initialization of the \code{timestamps} map, we have the same functionality as
before, but no memory leak.

\begin{shortlisting}
Map<T, Date> timestamps = new WeakHashMap<T, Date>();
\end{shortlisting}

Note that this same situation can hold even if you are able to modify the class
definition. A common scenario requires annotations on only a subset of all
instances of a class. In this case, is it not worth paying the memory cost to
have the ability to annotate every single instance. Therefore, this is another
case where a solution of side annotations, stored in a \class{WeakHashMap},
shines.

\subsection{Annotation Pool}

\subsection{Listeners}

\callout{listener-rule}{Listener Rule}{
Listener queues should weakly reference the callback object. This obviates to
maintain and debug code that explicitly deregisters the callback hook from the
listener queue.}


\section{Safety Valves}

\callout{soft-reference-rule}{Soft Reference Rule}{
Soft references must always be over values, not keys. Otherwise, testing
equality of keys will trigger a use of the reference. This will extend the
lifetme of the value, even though the only use of the entry was in checking to
see if its key matches another.	}

\section{Concurrency Issues}
\label{sec:lifetime-management-concurrency-issues}

If your program operates with many concurrent threads, you have to program
differently, because straightforward implementations of the above strategies will
result in concurrency issues. One of the primary problems will be lock
contention, as threads concurrently poll a reference queue. An important example
of this problem shows up in the implementation of a cache that can support many
concurrent users.\index{Caches, Concurrency Issues}

A cache is a map, usually of bounded size, with an eviction strategy for
maintaining that bound.\index{Caches} The Java standard library provides a
concurrent map implementation, in the form of the \class{ConcurrentHashMap}
class, but this is not a cache, because it has no eviction hooks with which one
can bound its size.

Following the soft reference rule, and using the basic guidelines for managing
reference queues from \autoref{sec:reference-queue-basics}, leads to a first
attempt at a \class{ConcurrentCache} implementation. You can extend the basic
concurrent hashmap, wrapping the map's values with soft references:
\begin{shortlisting}
class ConcurrentCache<K,V> extends ConcurrentHashMap<K,SoftReference<V>> {
   private final ReferenceQueue<V> refQueue = new ReferenceQueue<V>();
   
   protected void cleanupQueue() {
      SoftReference<V> v;
      while( (v = refQueue.poll()) != null) {
         remove(???); // oops!
      }
   }
}
\end{shortlisting}
Oops! This implementation provides no way to remove the key from the map, when
cleaning up the evicted entries. To fix this, you'll need to stash a pointer to
the key in the soft reference wrapper. It would be nice if the
\class{ConcurrentHashMap} implementation let you extend
its implementation so that its \class{\$Entry} class extended soft reference;
the \class{\$Entry} would serve this role perfectly. Instead, you have to
replicate this pointer structure, at a silly but unavoidable cost of memory
bloat. You can do so in a \class{CacheSoftReference} wrapper:
\begin{shortlisting}
class CacheSoftReference<K, V> extends SoftReference<V> {
   private final K k;
   
   public CacheSoftReference(K k, V v, ReferenceQueue<V> refQueue) {
      super(v, refQueue);
      this.k = k;
   }
}

class ConcurrentCache<K,V> extends ConcurrentHashMap<K,CacheSoftReference<K,V>> {
   private final ReferenceQueue<V> refQueue = new ReferenceQueue<V>();
   
   protected void cleanupQueue() {
      SoftReference<V> v;
      // poll causes lock contention!
      while( (v = refQueue.poll()) != null) {
         remove(v.k);
      }
   }
   
   public V put(K key, V value) {
      cleanupQueue();
      return super.put(key, new CacheSoftReference<K,V>(key,value,refQueue));
   }
   
   public V get(K key) {
      cleanupQueue();
      return super.get(key);
   }
}
\end{shortlisting}
This implementation still suffers from several critical problems. First, every
call to \code{put} must check the reference queue for pending evictions in order
to avoid unbounded growth of the eviction queue --- in the steady state,
\code{put} calls are likely to cause evictions. Even though \code{get} calls
won't cause evictions, in order to avoid pending evictions piling up as cached
elements are discovered to be unused, every call to \code{get} must also poll for
evictions. This can result in foiling the concurrency aspect of the
\class{ConcurrentHashMap}. Second, if the cache as a whole goes unused for a long
period of time, the pending evictions will pile up.

The only way to fix the lock contention problem, at least as of Java 6, is to
spawn a thread that periodically polls the reference queue for evicted entries.
This will also fix the second problem. This spawned thread's \code{run} method
will look just like the \code{cleanupQueue} method above, except that it should
loop indefinitely, and call \code{refQueue.remove()} rather than \code{poll()};
the former blocks until an eviction occurs (though you must still be careful to
check the return value for \code{null}, despite what the Javadocs claim).

At first sight, it would seem that you should be able to remove the calls to
\code{cleanupQueue} from the \code{put} and \code{get} methods. This, after all,
was the whole point of introducing the cleanup thread. However, this modified
implementation, while an improvement, suffers from a new problem. Now, if you
remove the \code{cleanupQueue} calls, when there is a large spike of \code{put}
calls in a short period of time, you are at risk of running out of Java heap due
to a large pileup of pending evictions.

You must have a safety valve in place to prevent this situation. One possibility
is to keep an approximate count of the number of \code{put} calls, and call
\code{cleanupQueue} only periodically. In order to avoid lock contention in
maintaining this count, you can do so in an unsynchronized way. There is still a
pathologic possibility that every racey increment of the put counter won't
actually increment the counter. If this worries you, you can use an
\class{AtomicInteger}, at increased expense. Instead of calling
\code{cleanupQueue} directly, the \code{put} method now calls a new
\code{helpCleaner} method:
\begin{shortlisting}
   static private final int SAFETY_VALVE = 1000;
   private void helpCleaner() {
      if (putCount.incrementAndGet() >= SAFETY_VALVE) {
         putCount.set(0);
         cleanupQueue();
      }
   }
\end{shortlisting}
There is no reason for \code{get} calls to call this method. The only point of
this safety valve is to avoid a sudden large influx of \code{put} calls. Indeed,
in this final implementation, the \code{ConcurrentCache} class needn't override
the \code{get} method of \code{ConcurrentHashMap}.
