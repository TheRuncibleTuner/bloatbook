\chapter{Scalability}

So that was all about small collections, and their problems. We kind of segwayed into the other half of the collection problem, which are collections that have high per entry costs. Typically large collections, but not always. Our design 3 really suffered from that. Basically, it has a high constant cost per entry. Just like our tree map did last week in the  health example. So each of these, just the per vertex and level cost here, is 48 bytes. It’s 28 bytes per enty in this hahsmap, and 20 for the pair objects.  It’s a constant cost that won’t be amortized. This is a theme that shows up in a lot of Java collections, in particular larger ones. Just a quick look at the per entry costs.
They are fairly high other than for arraylists. HashMap and HashSet is 28 or  36. ArrayList is the lowest I’ve seen. and LinkedList is 24 for again, this is purely because there is a separate object in the collection, delegating to your object. And that doesn’t include the cost of any boxing you have to do on your side to introduce a pair class, or a cap int, or anything like that.  That could easily double some of these numbers.

So it’s pretty common to see that in large collections, like in example we just saw. Another case is when you have nested small to medium sized collections, that still have enough entries to have high per entry costs. And this was the case in the SIP container that ws being used in the SAametime Presence server. This also had a requirement of maintaining a large number of subscriptions active at the same time. So this is handling stuff like when you’re sametime window is open, telling you who’s on line, what everyone’s status is. So this has to keep some huge number of statuses sessions active for various people. And so what they did here is the presence server employed the sip container to use a feature in websphere to store information in sessionstate about each of these subscriptions. So there’s quite a few layers in between, and basically they had 7 properties of each subscription they used to store. Basically, each of the 7 properties had a name, a string, and some kind of value, and most of the values were scalars. There some cases where they weren’t, but just for simplicity here. They just called the websphere, and websphere, said to add this to the sessionstate for each of these properties. And websphere stores the session state in some framework it has, as a hashtable of attributes and values. So it just stores each string with the values. Now fortunately, they did something really good here.   All of the subscriptions were storing the same 7 attributes so all the attributes were the same. At a higher level, presence server was sharing the strings. So at least they didn’t’ have 20,000 copies of the same seven property names, when they had 20,000 subscriptions. But what was costing the so much here the hashtable per entry costs, and the cost of boxing up all of these scalars, because that’s what websphere interface requires to store all these things. It was attribute, and cap object property. So this was something that was obscured by all the layers between. The decision is it’s very hard to say webshpere, fix this code for us, because that code is being used by all kinds of people, all over the place. So fortunately they came up with a pretty reasonable solution, which was that the presence server packaged all 7 of their properties into a single object, and then stored that subscription property object as a megaproperty, and that cut the per entry cost by 7. So that is an interesting hybrid example. Still relatively small collections, but it was really the per entry costs that were getting them. We’ve seen this a number of times.

So now just some special purpose cases of thes high per entry costs. That example from last week, Treemap of mapping doubles to doubles, and this was the real-world case that it came from. this is actually 1.2 or 1.4G structure. There were 52 treemaps, and in total there were 13 million entries in them. And the per entry costs was what 88 bytes per entry, again, same reasons. The alternative is to just use a collection that is optimized for scalars. This is super, super common. In fact there was a document processing application that where pretty similar kind of thing mapping Integers to Integers. In general collections of scalars suffer from these kinds of problems. And the solution is to have a specialized collection, outside the java collection for scalars. One of the issues is to standardize them somehow.

Another specialized collection is what’s called the Identity Map.  It’s for the case where your key is an object reference, and you know that’s going to be stable. In some sense this breaks your map interface standard, because Identityhashmap is a standard Java library cache, uses == rather than .equals to do its lookup. The common case is if I’m maintaining a proxy object for some other object, so I can make the proxy the key, and the vaue is the real object behind it. There’s always the pointer to the proxy that you want to use as the key. If this is stable, and your not counting on the value of it, then this is a great implementation. Just a little experiment I did, I tried implementing it as a standard HashMap, I put 10,000 entries into it, and then I use the Identty hashMap, and there was a pretty huge reduction, 59%, this was with the SUN JVM, IBM JVM was almost as good as SUN in this case. The main reason this was more efficient, I think isn’t just because of the ==, the main reason I sthey are using open addressing, rather than open chaining. They are not creating a different object for each entry. They actually have parallel arrays. In this case they have 1 array because they don’t need to store a key. They have one array that has the keys and values in it interspersed, if I remember. So this is a good class to know about, if you have a use case.

Just a summary of colllecitons.
There are really 2 classes of scaling issues. One is you have small nested collections, and you are paying the fixed cost of small collections, plus you may be paying per entry costs, if they have high per entry costs too for the small collections. The other case is the per-element costs of larger collections or nested collections, and any data delegation costs that those occur of the objects you store in them.

Just a little more about standard collections. The focus has been speed. Or supposedly it has been speed. It certainly hasn’t been footprint. There’s been very little attention given to that. I was a little surprised to see in the harmony classes. They harmony classes are the newest classes for J9, version 6. They have been open sourced on Apache. I was surprised that Harmony, I thought this was great, they finally fixed some of this stuff, and I found that they actually increased the footprint in a couple of cases. They added an extra field to arraylist, they made some of the sizes bigger. They cut the footprint in a number of cases too, but nothing substantial. I think the more disturbing part here, is the fact that a lot of the assumptions are hard wired. So there are not a lot of policy knobs to play with when you are coding to these things, and that’s really a problem. 

And finally, some of these specialized collections are worthwhile for their functionality, if not only for it’s footprint. The identity map is worthwhile for its footprint. Just some alternative collections. Like I said, the Apache commons collections are really nice from a functional perspective. They are not really doing much for footprint from what we can tell, at least from the code I’ve looked at. The Trove collections are great for footprint, but we are not allowed to use them, at least from what I understand. Anything you work on that may go out, you really have to check specifically.   The opensource licensing stuff, it’s on a product by product basis. There’s a lot of collection class efforts Amino classes are aimed at concurrency, Javalution from Raytheon is for soft realtime, and externally on sourceforge, Cliff Click has some non-blocking collections. Actually, he claimed the footprint is good. I haven’t seen any studies on it. The best source of good collections I’ve seen are actually scattered around the IBM frameworks. Portal, Rational, just all over the company.

Developers have a tough situation. Choosing the collections not always obvious what the choices are. Purpose, to raise the awareness. What kinds of things to think about, what to measure. Not to think that things are cheap. And in this context of expensive implementations, sometimes you do hit walls, and there’s nothing you can do. But sometimes you really can fix a number of problems. There’s a lot of problems that have easy solutions. Choosing the default size, lazily allocating collections that are most likely to be empty. Picking a collection with less functionality if you don’t need the full functionality. I strongly advise not writing your own collections, unless you really have search the company for something that will do the job, because it just introduces a whole other set of problems.

Implementing a cache, relationships, a nested map, attribute-value pairs etc – global optimizations. Study – what’s most of the heap?  Which kinds of database bulk storage optimizations would be appropriate?


Chapter: SPECIAL COLLECTIONS


Multikey

I said this was a pretty common programming idiom, and in fact I was looking around the Apache commons collections, which is a fairly nice, open source set of collections, they have something called a multikey map. Great, maybe they are solving this already. They have, like every other collection, they have a wrapper object, and then they have an array, and inside that it’s an array of these multi-key objects, and I thought this is nice, you’ll subclass this thing, and then you’ll be done. Instead what they do is that in turn delegates to another array, which has the different parts of the key in it, and your keys get added to that. So in fact, this is even more expensive, maybe it’s about the same, because it has a wrapper multikey which is equivalent to an arraylist object, and then the array is just like the array that was inside the array list. So the could have easily implemented something. There are calls in this class, there’s a put call with 2 keys, and a put call with 3 keys, and a put call with 4 keys, so they could have easily have specialized this, and had a multikey 2, and a multikey3, and so forth that were subclasses of multikey, that had the right number of fields in it. But, it was just the focus here. SO that’s another 20bytes per entry in this case also. Frankly, as I looked around at a lot of the standard collections out there, there’s just no attention to footprint. 

CONCURRENT MAPS

So now some special cases in this. There are a lot of different uses of this, and one interesting things we’ve been learning, as we’ve been looking at all these case studies is that we’re really trying to think about why are people using collections, and what are they using them for? In the last example, we saw, they are using them to implement a map. A fairly complicated map. In that first long example. 

This example was from one fo the SameTime products, Lotus server-side products. I have a lot of Lotus examples here, only because we’ve worked with the a lot. Not because their code is worse than anyone elses. This is a pervasive problem. 

So in this case, this is the sametime gateway actually, and what it’s doing is keeping track of active chat sessions. Example is a bit simplified. They need to handle some gigantic number of concurrent sessions. So they use a concurrent hashmap, the standard Java concurrent hashmap, and in that they store sessions. Each session has some number of subscribers, and so within each session they use a concurrent hashmap to store the subscribers. When we look at this together, everyone was in shock that this concurrent hashmap thing was so huge. And again this was a relatively small run, but even for this, we didn’t expect there to be 177MB of concurrent hashmap. And so we looked at it, and it turns out they took all the defaults for concurrent hashmap, and each concurrent hashmap costs 1700 bytes, approximately. The reason for that is that the concurrent  hashmap is really designed for high concurrency, so it contains 16 parallel hashmaps by default, each with 16 elements. Now if you only have a few subscribers, most of that stuff is empty. So when the developers saw this, they immediately realized, we don’t need that kind of concurrency control. We don’t have thousands of subscribers on the same session. We need that concurrency on a higher level. Sure, we need a concurrent hashmap up here to manage hundreds of thousands of sessions, but each session is only going to have a few subscribers. So we can greatly limit the number of concurrency we can support there, as long as it’s thread safe. So one developer suggested, ok , let’s just take the concurrent hashmap, and choose the default as 3 instead of 16. With that, they could have gotten a reduction of 67%, which is substantial, but then one of the other developers suggested let’s just use HashTable, because it’s much, much smaller, and it’s still threadsafe. So hashtable was a great choice for the sametime people, as it turned out.

UNMODIFIED MAP

So here’ another example, of unmodifiable, now this was from NetFlix, which Nick analyzed, it was an IBM customer, and in this case, we don’t know exactly what this was, some kind of cache of titles they maintained, and it had also a 2-level structure to it, and it had in this particular run 2 million of these inner maps, taking up about .5G, and when we looked inside there, what we found, was that each of those maps was an unmodifiable map, wrapping a smaller hashmap, so they had 2 problems. One was why they had the small hashmaps, and that’s a separate issue, but even the unmodifiable hashmap was pretty substantial. This was a 64 bit JVM, I think this was a SUN JVM, so the cost was even double of what we say before, it was 56 bytes for each of these wrappers. It was a pretty hefty cost. So this is the kind of thing where – this is a functionality that. Unmodifiable is pretty useful for development time, to avoid programming errors, but it’s not clear that especifallly, this fine grain, that in deployment you would want to keep a feature like that in there, this is pretty expensive.  

There’s a whole set of Java wrapped collections, for type checking, for modifiable, and that’s the general pattern. That’s a nice programming pattern, and it certainly simplified the class hierarchy. I read some of the notes from the designers of these collection classes, about why they made these choices, and they said it would just have complicated the class hierarchy in the Java collection classes to have all these combinations there. And that makes perfect sense when you have a few large collections. But in this kind of case, when you have lots and lots of small collections, it really adds up. On this JVM, this is 1.4.2, maybe this we sun 1.5 the cost is 28 bytes per each of these wrapper objects, which is pretty substantial. So that’s including its header, its bookkeeping, and the pointers. That’s ok for large collections, for small collections, it really adds up. 


