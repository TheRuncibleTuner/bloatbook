
\chapter{Introduction}
\label{chapter:introduction}

Managing your Java program's memory couldn't be easier, or so it would seem.
Java provides you with automatic garbage collection, and a compiler that
responds to your program's operation. There are lots of libraries and frameworks
available, written by experts, that provide powerful functionality. All you have
to do is piece together the parts and let the Java runtime system do the rest
for you.

The reality, unfortunately, is very different.  If you just assemble the parts,
take the defaults, and follow all the good advice to make your program flexible
and maintainable, you will likely find that your memory needs are much higher
than imagined. You may also find that precious memory resources are wasted
holding on to data that is no longer needed, or even worse, that your system
suffers from memory leaks. All too often, these problems won't show up until
late in the cycle, when the whole system comes together. You may discover, 
for example, when your product is about to ship, that your design is far from fitting into memory, 
or that it does not support nearly the number of users it
needs to support.  Fixing these problems can take a major effort, requiring
extensive refactoring or rethinking of architectural decisions, such as the
choice of frameworks you use.  

Alternatively, a methodology based on sound design can
prevent this type of scenario from happening. Memory usage, like any other
aspect of software, needs to be planned. At its core, programming is an engineering
discipline, and there is no escaping the fact that the consumption of any finite
resource must be measured and managed.


\section{A Short Quiz}

 To start you thinking about memory
consumption, here is a quiz to test how good you are at estimating sizes of Java objects. 
(Assume a 32-bit JVM.) If you look inside a typical Java heap, it is mostly filled with the kinds of
objects used in the quiz --- boxed scalars, strings, and collections.
\begin{verbatim}

Question 1.  What is the size ratio in bytes of Integer to int?
   
      a. 1:1
      b. 1.5:1
      c. 2:1
      d. 4:1
      e. 8:1
      
\end{verbatim}

\begin{verbatim}
   
Question 2.  How many bytes in an 8-character String?

      a. 8 bytes
      b. 16 bytes
      c. 20 bytes
      d. 40 bytes
      e. 56 bytes

\end{verbatim}

\begin{verbatim}             
Question 3.  Arrange the following 2-element collections in size order:
    
      ArrayList, HashSet, LinkedList, HashMap
      
\end{verbatim}

\begin{verbatim}      
Question 4.  How many collections are there in a typical heap?
   
      a. between five and ten
      b. tens
      c. hundreds
      d. thousands
      e. two or more orders of magnitude bigger than the above
      
\end{verbatim}

\begin{verbatim}
Question 5.  What is the size of an empty ConcurrentHashMap?
   
      a. 17 bytes
      b. 170 bytes
      c. 1700 bytes
      d. 17000 bytes
      e. 500 bytes
                 
\end{verbatim}

\newpage

Question 1.  Correct answer: d.

\begin{quote}
Every time
a program instantiates a class, there is an object created in the heap, and as
shown by the 4:1 size ratio of \class{Integer} to \class{int}, objects are not
cheap.
\end{quote}

Question 2.  Correct answer: e.

\begin{quote}
Strings often consume 40-50\% of the heap. They are
surprisingly costly. If you are a C programmer, you might think that an 8-character string should 
consume 9 bytes
of memory: 8 bytes for characters and 1 byte to indicate the end of the string.
What could possibly be taking up 56 bytes? Part of the cost is because Java uses
the 16-bit Unicode character set, but this accounts for only 16 of the 56 bytes.
The rest is various kinds of overhead.
\end{quote}

Questions 3.  Correct answer: ArrayList, LinkedList, HashMap, HashSet.

\begin{quote}
After strings,
collections are the largest consumers of memory in most applications. Surprisingly, a
\class{HashSet} is bigger than a \class{HashMap}, even though it does less.
This is an interesting fact that will be explained later.
\end{quote}

Question 4.  Correct answer: e.

\begin{quote}
In typical real programs, having hundreds of thousands, or even millions of collection
instances in a heap is not at all unusual. If there are a million collections in
the heap, then the collection choice matters. One collection type might use 80
bytes more than another, which may seem insignificant, but in a production
execution, a few wrong choices can add hundreds of megabytes.
\end{quote}

Question 5.  Correct answer: c.
\begin{quote}
\class{ConcurrentHashMap}, compared to the more common
collections in the standard library, is surprisingly expensive. If you are used to creating
thousands of \class{HashMaps}, then you might think that it is not a problem to
create thousands of \class{ConcurrentHashMaps}. There is certainly nothing in
the API to warn you that \class{HashMaps} and \class{ConcurrentHashMaps} are
completely different when it comes to memory usage. This example shows why
understanding memory costs is important.
\end{quote}

The quiz gives some sense of how surprising the sizes are for the Java basic
objects, like boxed scalars, strings, and collections. When code is layered
with multiple abstractions, memory costs become more and more difficult to
predict.

\section{Facts and Fictions}

In addition to the technical and engineering challenges that managing memory
pose, there are other reasons why memory problems are so common. In particular,
the software culture and popular beliefs can lead you to ignore memory costs. 
Some of these beliefs are actually myths.  Here are several.

\begin{comment}
\callout{myth0}{Memory is Cheap.}{
Since memory is so cheap, you do not have to think of memory as a finite
resource.}
Buying more memory at the last minute to fix a severe scalability problem is rarely an option, especially
if the off by an order of magnitude from what was planned.  And no
amount of additional memory will fix a memory leak over time. Adding a large amount of memory can also have
a negative impact on performance, such as on the operation of the garbage collector, and could hit up against physical
contraints in the system architecture.
\end{comment}

\callout{myth1}{You Can Ignore Memory In Java.}{
Java's garbage collector and JIT will optimize your memory usage.
To avoid code complexity, you should not even think about how much memory is
being used. }

There is a widespread belief that objects are free, at least temporaries are, 
and everything will be taken care of for you by the garbage collector and the JIT compiler.
Given all of the research on garbage collection and runtime optimization, this
is not a surprising perception. Therefore, much of software engineering focuses on
design from a maintainability standpoint, and assumes the performance will
magically be taken care of.

In fact, all of the commercial JITs
we know of do absolutely nothing in terms of storage optimization, so
that every single object, with all of its fields, ends up as an object in the
heap, taking up space.\footnote{Some JITs do optimize a
small percentage of temporaries by allocating them on the stack.} 
%In a later chapter we'll look into the details of what that means.
Yes, garbage collectors do a good job of cleaning up temporary objects, though having a large number of
temporaries will still require extra space and time. And many objects are not
temporaries. It is these longer-lived objects that cause memory problems. 


\callout{myth2}{Libraries and Frameworks are Optimized.}
{Another common myth is that the libraries and frameworks are already optimized,
because these were written by experts.
} 

Standard libraries and frameworks are usually optimized for speed, not
necessarily memory usage. Even when framework authors try to pay attention to
memory, they often have a hard time predicting what the usage of their
framework is going to be. Sometimes they guess wrong and sometimes they don't
know where to start.  So it's very unlikely that a framework has been optimized for
your particular use case.  And of course, the nesting of frameworks results in
layers of unoptimized memory costs.

\callout{myth5}{There are No Memory Leaks in Java.}
{Since Java has a garbage collector, it isn't possible
to have a memory leak.
}

Java has a managed runtime. An important
part of this is automatic garbage collection. The standard sales pitch
claims that manual reclamation of memory isn't required, and, for
the most part, this is actually true. Memory leaks occur frequently in
languages like C++, where the programmer can easily forget to free an
object when the variable pointing to it goes away.  However, in Java, the
garbage collector automatically finds free objects. So how can you get a
memory leak?

Problems arise when long-lived data
structures hold on to objects longer than they are needed, and these structures
continue to grow indefinitely. The garbage collector cannot collect objects
when it thinks they are still needed. If structures continue to grow without
bound, eventually you will run out of memory. This is, in fact, a memory leak, 
and can be a particularly hard problem to solve. Deciding how long each object
should stay in memory is a necessary part of the developer's job, even with a
managed runtime. The second part of this book deals extensively with
this topic.

\callout{myth4}{Improving Memory Usage Hurts Performance.}
{There will always be a performance/memory tradeoff. If you improve memory
usage, then the application will run slower. 
Similarly, if you improve performance, the memory usage will suffer.
} 

This is a false dichotomy in a lot of people's minds: if an application is
using a lot of memory, then it must be buying you something in terms of performance. 
But in reality, that is not always the case. In fact, sometimes bad memory usage will
result in poor performance as well. For example, if 
the heap is holding on to a lot of long-lived objects, then there is very little
headroom for temporaries, and so the garbage collector has to run much more
often.  Similarly, if there are too many long-lived objects, then your cache may
be too small for optimal performance.

\callout{myth3}{Nothing Can Be Done.}
{Java is expensive, and there is nothing you can do about it.
This is just the cost of object-oriented programming in Java.
}

The purpose of this book is to show that there are
many techniques that you can employ to improve memory usage, armed with some
knowledge of what things cost. There is almost always something you can do that
will make a difference!



\section{\thetitle}
\paragraph{Problem-focused}This book presents a practical approach to making
effective use of memory.
%The main focus is on how to represent your data models efficiently. It also
%addresses managing the lifetime of data, which is a common source
%of errors such as memory leaks. 
It is organized according to
the most common design problems and patterns that have an impact on memory usage. For each pattern we point out
traps, provide solutions, and help you compare the costs of alternative
designs.
%examples
%tradeoffs
%We guide you in estimating memory costs of your own classes and
%off-the-shelf libraries in the context of typical usage. 
While the book, as a whole, gives you a
comprehensive approach to memory usage, it's designed so that you can pick and choose what you need when you need it.
That way, you can focus your efforts where you'll get the most benefit right away, no matter what
part of the development cycle you're in for a particular part of your system.
This approach fits nicely with different styles of development, including agile.

\paragraph{Estimation}The book covers both parts of the estimation problem: what does a
data structure cost at a particular size, and how will it scale up. The
estimation techniques will help you focus your optimization efforts where they matter most.
It's also easy to make things worse when
optimizing, if you're not careful, since there are so many hidden memory costs
in Java. At relevant points we'll give you formulas to
determine whether an optimization is worthwhile given the characteristics of your data.

\paragraph{Java mechanisms}Interspersed throughout the book are a look at some
elements of Java that have an impact on memory. We'll look inside some library
classes such as strings and collections, to give you insight into how these and
similar classes make use of memory.
We'll also look at lifetime management mechanisms, such as weak and soft references,
and show how best to use them.

\paragraph{Requirements}In practice, many problems that look like implementation
errors are actually caused by an incomplete understanding of requirements. Throughout the book we
give you questions to ask about your data. For example: Do you need random access into this data? Will entries ever be deleted from these collections? Is the lifetime of this object tied to another object or event? Which
collections will grow larger as various aspects of load increase?  Asking these
questions can help you choose efficient representations that are specialized for your application's use cases.
They can also help you predict scalability, and avoid lifetime management bugs
like memory leaks.


% probably overkill:
%\dividingline

%\subsection{Estimating the Efficiency of Your Design}
%discuss estimating (Edith text on counting bytes, etc is great) (including
%   scalability discussion) (and focusing on important stuff)
%discuss health very briefly
%discuss entities and collections

\paragraph{Roadmap}\autoref{part:1} is about designing and implementing
memory-efficient data models.
\autoref{chapter:memory-health}, \nameref{chapter:memory-health}, lays the foundation with two
concepts that are used throughout the book. If you read one chapter first,
this is the one to focus on.
It shows you how to compute an accounting metric,
the \emph{Bloat Factor}, which measures the density of actual data in your
data models. Memory spent on everything else, such as
Java's internal object headers, is the overhead of the representation.
Poor designs waste more space on these overheads, and thus have a high bloat factor. \autoref{chapter:memory-health} also
introduces a diagramming technique,
the \emph{Entity-Collection Diagram}, that makes it easy to see memory costs and overheads in a design,
and to compare design alternatives.
It separates out the two kinds of memory consumers:
% highlights the major elements of the data model implementation, so that the
% costs and scaling consequences of the design are easily visible.
% These diagrams appear throughout the book to illustrate various implementation
% options and their costs.
%
% the implementation of
the application's \emph{entities}, such as the
business objects, and the choice of collections used to access them. Entities and collections have
different patterns in the way they use memory, and they require different solutions.

Chapters~\ref{chapter:delegation} through
\ref{chapter:representing-values} cover the design of
entities. \autoref{chapter:delegation} is about estimating memory costs. It covers the basic costs of fields
and objects, and the costs of constructing entities from multiple classes.
\autoref{chapter:field-patterns} covers efficient patterns for laying out classes from fields.
\autoref{chapter:representing-values} compares alternative representations for common field datatypes. 
\autoref{chapter:sharing-immutable-data} provides techniques for
reducing the amount of duplicate data.

Chapters \ref{chapter:brief-introduction-collections} through
\ref{chapter:dynamic-records} are about
using collections. \autoref{chapter:brief-introduction-collections} is
an overview of collection resources and costs.
%in the standard library and open source frameworks
The next three chapters cover the various uses of collections:
as one-to-many relationships (\autoref{chapter:representing-relationships}),
as large access structures such as indexes and tables
(\autoref{chapter:tables-indexes}), and as dynamic records and attribute maps
(\autoref{chapter:dynamic-records}).

\begin{comment}
[NMM 20120628 too much E-C diagram detail for intro?]
A data model implementation begins with a conceptual understanding of the
entities and relationships in the model.  This may be an informal understanding,
or it may be formalized in a diagram such as an E-R diagram or a UML class
diagram.  At some point that conceptual model is turned into Java classes that
represent the entities, attributes, and assocations of the model, as well as any
auxiliary structures, such as indexes, needed to access the data.  The example
below shows a simple conceptual model, using a UML class diagram.  A Java
implementation of that model is also shown, using rectangles for classes and
arrows for references.  %The Java diagram below is typical of a schema diagram,
in that it shows
\end{comment}

% probably overkill:
%\dividingline

\autoref{part:2} is about managing the lifetime of objects. In
\autoref{chapter:lifetime-requirements}, you will learn how to establish the
lifetime requirements of your data, mapping your needs to one of a number
of common lifetime patterns.
For example, some objects' lifetimes need to be correlated with
related objects; others may have their lifetimes extended to buy
performance by avoiding recomputation.
%Many problems can be avoided by carefully
%understanding the reasons for retaining data in memory.
\autoref{chapter:lifetime-fundamentals} provides a primer on the memory
management facilities that Java offers, to help you implement the lifetime
requirements of your data.

%\autoref{chapter:lifetime-implementation-strategies} and
%\autoref{chapter:trading-space-for-time} focus on the last two of these lifetime
%patterns.

% probably overkill:
%\dividingline

%In addition to determining the bloat factor of individual data structures and
%avoiding memory leaks, it is important to estimate how well the overall design
%will scale up as load increases. 
%The final part of this book, 
\autoref{part:3} covers issues related to scalability.
\autoref{chapter:estimating-scalability} teaches strategies for estimating
how a design will scale up as load increases.
It builds on the Entity-Collection Diagram and Bloat Factor introduced in
\autoref{chapter:memory-health}.
% It will help you assess whether your design will fit into memory.
\autoref{chapter:large-long-lived} covers cases when your data model does not fit, even after
applying the tuning methodology of \autoref{part:1}. This chapter presents
bulk storage techniques for achieving greater scalability, without
resorting to the use of secondary storage. This approach remains
within the confines of Java, but gives up many of the benefits of object
orientation.

The memory estimates in the examples are from the 32-bit \oracle
\javasix \jre.
\autoref{chapter:jre-comparison} is a reference for estimating memory costs,
for some common \jres and architectures.
\autoref{chapter:jre-options} gives memory configuration options for these same
\jres.


%Finally, \autoref{chapter:secondary-stores} gives strategies for the case when
%you must resort to the use of disk or remote storage in order to make your data
%fit.



\begin{comment}
 % NICK's NOTES
microscopic estimating (field-level counting), and accounting to predict
scale

EC diagram, bloat factory, scaling, 

lifetime: sometimes it's not a leak, just a consumption problem (in-memory
design, didn't fit)

sometimes optimizations cost: making a side object for rarely used fields; when
is it useful sharing immutable data (how much sharing before factoring it out
is worthwhile)

what java gives you, what it doesn't, some things are kinda hard to use; java
makes it hard for you

you actually have control



preface

things are hard to change at some point. it's easy to get to an irreconciliable
spot.

intro

why isn't this just for systems programmers making b-trees? this is about
everyday application development

i have GC, why care about lifetime?
i have standard data structures, why care about data model design?

data modeL: entites and relations

visual presentation of density
[flyweight: canonicalizing map]
\end{comment}