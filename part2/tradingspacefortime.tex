\chapter[Trading Space for Time]{Trading Space for Time: Caches, Resource Pools and Thread-Local
Stores}
\label{chapter:trading-space-for-time}

Sometimes, you need to consume more memory, not less, in order to make your
application run well. There are two main reasons to trade off increased memory
consumption for increased overall application performance: caching and cloning.
A cache stores the output of expensive operations. A resource pool is a cache
whose elements are interchangeable. For example, a connection pool caches the
outcome of establishing of a connection to a remote data source. A memory pool
amortizes the costs associated with allocation and reclamation of memory. These
are caches, except that each connection to a particular machine, or each
allocation of a particular size or type, is interchangeable with any other.
Finally, cloning allows concurrent, rather than synchronized, access to data.
All of these mechanisms trade space for time. They require extra memory, say, on
top of establishing a new connection for every remote access. But this trade-off
often pays off when the increase in performance is more important than increase
in memory consumption.

When you implement your own time-space trade-offs, you will leverage underlying
mechanisms provided by Java. Soft references and \tls can help with caching,
resource pooling, and cloning, but misuse of them can result in memory issues
such as leaks and memory drag.

%There are three important cases of time-space tradeoffs. The first covers
%the situation where recomputing attributes, rather than storing them, is a
%better choice. The next two cover situations where spending memory to extend
%the lifetime of certain objects saves sufficient time to be worthwhile: caches
%and resource pools.

\section{Patterns Based on Soft References}
\label{sec:soft-reference-details}
\index{Soft References}

Java offers soft references as way to manage bounded-size caches.
\autoref{sec:soft-references} introduced soft references. You can use soft
references to reference objects in place of normal references via instance
fields. When you do so, you are informing the \jre that this reference should
not inhibit the object from being reclaimed, but that it should only be
reclaimed when heap memory grows tight.

To use soft references, you will employ the \class{SoftReference} class. The
constructor for a \class{SoftReference} takes, as a parameter, the object you
would like to softly reference. That object lives as it normally would until the
point in time when it is only reachable by traversing soft (and possibly also
weak) references. This means that the there is no way to reach the object from
your code, as you traverse fields and local variables, through only normal
(strong) references; all chains of traversals must go through at least one soft
reference, and possibly weak references, too.

While an object is softly reachable, the \jre will keep it around, for as long
as there is enough free Java heap. At least, this is what the Java language
specification requires. The only requirement imposed by the language
specification is that the \jre must clear all soft references before it throws
an \class{OutOfMemoryException}. In practice, most \jres also impose a freshness
criterion on softly reachable objects. These \jres will first discard those
softly reachable objects that have been least recently used. Most, if not all,
\javafive and \javasix \jres use a rough approximation a least recently used
(LRU) eviction policy. Early \jres tended to make poor decisions, when choosing
how to clear soft references. One such \jre would wait until the heap was
exhausted, at which point it would clear all soft references. On early \oracle
\jres, a soft reference behaves like a weak reference, which means that such
softly referenced objects will probably be collected in the very near future,
independent of demand for memory, or how recently used the referenced objects
are. \autoref{tab:soft-reference-by-jre} summarizes the treatment of soft
references by various \jres.

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
soft reference clipping policy & \jre versions \\
\cmidrule(r){1-1} \cmidrule(l){2-2} \\
LRU heuristic & \ibm 1.4 and higher, \oracle 1.3.1 and higher \\
clip all when memory is tight & \ibm prior to 1.4 \\
treat as weak references & \oracle prior to 1.3.1 \\
\bottomrule
\end{tabular}
\caption{Depending on your \jre, you may not be able to employ soft references
as you might hope. Only some \jres discard
of soft references in an LRU-like fashion.}
\label{tab:soft-reference-by-jre}
\end{table}

\paragraph{Be Cautious with Soft References}
Most contemporary \jres measure freshness relative to the amount of free memory.
For example, the \oracle \jres, after version 1.3.1, the default behavior is to clip
all soft references that haven't been used for 1 second, for each megabyte of
free heap. This means that, if you have 500MB of free heap, any cache entries
that haven't been used in 6 minutes will be evicted after the next garbage
collection. This is true, even if the total size of your heap is 600MB: despite
the heap being 83\% unoccupied, the \jre will begin to evict some cache entries.
This value can be tweaked, at least on \oracle \jres, via the command-line argument
\code{-XX:SoftRefLRUPolicyMSPerMB}. This value is in milliseconds, so passing
5000 would change the above eviction example from 6 minutes to 30 minutes (since
the default is 1000 milliseconds per megabyte of heap).

This is a reasonable approximation to a true LRU eviction policy. For those
\jres that use some a freshness heuristic, soft references can still form the
basis for implementing caches. However, you must not depend on this heuristic
blindly.


%To traverse a soft reference, and access the underlying object, you can invoke
%the soft reference's \code{get} method. To keep track of freshness, most \jres
%store the last time \code{get} was called in each \class{SoftReference}
%instance.  



\begin{wrapfigure}[7]{l}{0.395\textwidth}
\centering
\begin{framedlisting}
<refs soft="27801" weak="3" phantom="0" ... />
\end{framedlisting}
\caption{Verbose garbage collection data from an \ibm \jre tells you
if a misuse of references is causing a pile-up of reference objects.}
\label{fig:verbose-gc-output}
\end{wrapfigure}
You should monitor the behavior of soft references in your application. On an
\ibm JVM, you can see whether soft references are piling up: enable verbose
garbage collection (by adding \code{-verbose:gc} to the command
line\index{Verbose Garbage Collection}), and track the output of the form shown
in \autoref{fig:verbose-gc-output}. If the LRU heuristic is not working well,
then you will observe a ragged sawtooth pattern in the number of soft
references. If so, then you must not depend on soft references for implementing
your caches! On \oracle \jres, you should also monitor the hit rate of you caches,
relative to the value of the \code{-XX:SoftRefLRUPolicyMSPerMB} tuning
parameter.

\paragraph{Consider Establishing a Bound, Too}
Because of the potential limitations of the LRU heuristic, you may want to treat
soft references as more of a safety valve, rather than as the primary means for
ensuring that the cache stays at a reasonable size. With soft references only,
it is possible that your cache will be entirely cleaned up, if there is a lull
of only a few minutes. With only a pre-established capacity bound on the data
structure, you avoid this problem, but are subject to memory exhaustion. If you
choose a bound of, say, 10,000 entries, you do have some assurance that this
will indeed by the occupancy level, in the steady state. However, unless you do
the careful analysis described in \autoref{part:1}, combined with the
scalability analysis described in \autoref{part:3}, you have no way of knowing
how many bytes those 10,000 entries consume. If the unit cost of each entry of
the cache is 100 kilobytes, then you cache, in the steady state, will occupy 1
gigabyte of heap. You may be left with excessive garbage collection, because you
have left little room for temporary object creation. You may even suffer out of
memory exceptions under certain conditions.

Soft references can serve as a safety valve, to protect against these memory
problems. With some careful experimentation, you should be able to combine
capacity bounds with soft references to achieve the desired effect.

\paragraph{What to Reference, Softly}
If you do use soft references to help bound the size of a data structure, you
need to be cautious about how you use soft references. If you reference the
wrong objects, using soft references, you may foil the LRU heuristic of the
\jre. For example, say you are using a \class{HashMap}, and decide to softly
reference the keys. Whenever the map has hash collisions, then each call to
\code{get} or \code{put} may update the last-used field of the soft reference
for objects that are not \code{equal} to the given key, but that hash-collide
with it.

\callout{soft-reference-rule}{Soft Reference Rule}{
Soft references must always be over values, not keys. Otherwise, testing
equality of keys will trigger a use of the reference. This will extend the
lifetime of the value, even though the only use of the entry was in checking to
see if its key matches another.	}

\subsection{Example: Implementing a Concurrent Cache}
\label{sec:lifetime-management-concurrency-issues}

A cache is a map, usually of bounded size, with an eviction strategy for
maintaining that bound.\index{Caches} The Java standard library provides a
concurrent map implementation, in the form of the \class{ConcurrentHashMap}
class, but this is not a cache, because it has no eviction hooks with which one
can bound its size.

\begin{wrapfigure}[9]{l}{0.34\textwidth}
\centering
\vspace{-2mm}
\begin{framedlisting}
new MapMaker().concurrencyLevel(4).softKeys().maximumSize(1000).createMap();
\end{framedlisting}
\caption{Using Google's Guava library to create a concurrent cache.}
\label{fig:mapmaker-make-concurrent-cache}
\end{wrapfigure}
The Java standard library does not provide a concurrent cache implementation.
Thankfully, there are other libraries at your disposal, such as \class{MapMaker}
from the Google Guava libraries~\cite{google-guava}. With \class{MapMaker} you can create
a concurrent cache by calling the sequence in
\autoref{fig:mapmaker-make-concurrent-cache}. It is nonetheless useful to step
through what it takes to make this work. In doing so, you will learn about
managing soft references and reference queues.
% One of the primary problems you
%will run into is lock contention, as threads concurrently poll a reference
%queue.
\index{Concurrency}\index{Caches}

\begin{wrapfigure}{r}{0.605\textwidth}
\centering
\begin{figurelisting}
class Cache<K,V> {
  ConcurrentHashMap<K,SoftReference<V>> m;
  ReferenceQueue<V> q;

  void put(K k, V v) {
    cleanupQueue();
    m.put(k, new SoftReference(v, q));
  }
  
  void cleanupQueue() {
    Reference r;
    while((r = q.poll()) != null)
       m.remove(???); // oops!
  }
}
\end{figurelisting}
\caption{A first attempt at a concurrent cache.}
\label{fig:concurrent-cache-first-attempt}
\end{wrapfigure}

\paragraph{Eviction Mechanisms}
To implement the eviction aspect of a cache, you can leverage soft references.
You need to follow the soft reference rule from earlier in this chapter: softly
reference the values, never the keys. You can extend the basic
\class{ConcurrentHashMap}, wrapping the map's values with soft references. You
also need to follow the reference queue polling rule from
\autoref{reference-queue-rule}: poll the queue at least as often as soft
references are created --- in the steady state, \code{put} calls are likely to
cause evictions. This leads to the first attempt at a concurrent cache
implementation, shown in \autoref{fig:concurrent-cache-first-attempt}. The first
thing you will discover is that your cache implementation cannot inherit from
the concurrent hash map with softly reference values, because then the
\code{put} and \code{get} methods would have to expose the soft references.
Instead, your cache must delegate to the underlying map. 

Unfortunately, this doesn't quite work. The variable \code{v} references the
value, not the key. Therefore, this first implementation provides no way to
remove evicted value from the map. To fix this, you'll need to stash, in the a
subclass of the soft reference wrapper, a pointer to the key.\footnote{It would
be nice if the \class{ConcurrentHashMap} implementation let you extend its
implementation so that its \class{\$Entry} class extended soft reference; the
\class{\$Entry} would serve this role perfectly.} Then, you can update the
\class{Cache} implementation to extend
\class{ConcurrentHashMap<K, CacheValue<K,V>>}, as shown in
\autoref{fig:concurrent-cache-second-attempt}.


\begin{wrapfigure}{r}{0.43\textwidth}
\centering
\begin{framedlisting}
class CacheValue<K, V> extends SoftReference<V> {
  K key;

  CacheValue(K k, V v, ReferenceQueue<V> q) {
    super(v, q);
    this.key = k;
  }
}
\end{framedlisting}
\caption{You will need a special value reference that keeps a reference to the
key, to allow you to remove the entry when it is evicted from the cache.}
\end{wrapfigure}

\paragraph{The Need for a Cleanup Thread}
This updated implementation still suffers from two critical problems. First, the
reference queue \code{poll} method is internally synchronized. Every call to
\code{put} and \code{get} must poll the reference queue at least once, to avoid
unbounded pile-up in the queue. This can result in severe lock contention, which
pretty much foils the concurrency aspect of the \class{ConcurrentHashMap}.
Also, if the cache as a whole goes unused for a long period of time, then the
objects already queued up in the reference queue will suffer from memory drag
(see \autoref{drag}). These queued up objects will never be polled, since
neither \code{put} nor \code{get} will be called. The good news is that this
won't result in a memory leak, where objects pile up without bound, but it is
still a potential source of memory problems. You may find that you are wasting
quite a bit of memory in the reference queues of quiescent caches.


\begin{wrapfigure}[18]{l}{0.61\textwidth}
\centering
\begin{figurelisting}
class Cache<K,V> {
  ConcurrentHashMap<K,CacheValue<K,V>> m;
  ReferenceQueue<V> q;
   
  void cleanupQueue() {
    Reference r;
    while( (r = q.poll()) != null)
       remove(((CacheValue)r).k);
  }
   
  void put(K k, V v) {
    cleanupQueue();
    m.put(key,new CacheValue<K,V>(k,v,q));
  }
}
\end{figurelisting}
  %public V get(K k) {
    %cleanupQueue();
    %return super.get(k);
%  }
\caption{A second attempt at a concurrent cache.}
\label{fig:concurrent-cache-second-attempt}
\end{wrapfigure}

One way to fix the lock contention problem is to have a separate thread be
responsible for polling the reference queue. This will also fix the second
problem, as long as the polling thread is active for as long as the cache itself
is around. This spawned thread's \code{run} method will look just like the
\code{cleanupQueue} method above, except that it should loop indefinitely.

\paragraph{Avoiding Spikes}
At first sight, it would seem that you should be able to remove the calls to
\code{cleanupQueue} from the \code{put} and \code{get} methods. The cleanup
thread should take care of all necessary polling, right? However, when there is
a large spike of \code{put} calls in a short period of time, you are at risk of
running out of Java heap due to a large pileup of pending evictions.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\begin{framedlisting}
AtomicInteger putCount;
void cleanupQueueHelper() {
  if (putCount.incrementAndGet() >= 1000) {
    putCount.set(0);
    cleanupQueue();
  }
}
\end{framedlisting}
\caption{To avoid spikes, \code{put} must call this method.}
\label{fig:new-cleanup-queue}
\end{wrapfigure}
You must have a safety valve in place to prevent this situation. One possibility
is to keep an approximate count of the number of \code{put} calls, and call
\code{cleanupQueue} only periodically. In order to avoid lock contention in
maintaining this count, you can do so in an unsynchronized way. There is still a
pathologic possibility that every racey increment of the put counter won't
actually increment the counter. If this worries you, you can use an
\class{AtomicInteger}, at increased expense. A new \code{cleanupQueueHelper}
method is shown in \autoref{fig:new-cleanup-queue}, and \code{poll} now calls
this method. There is no reason for \code{get} calls to call this method. The
only point of this safety valve is to avoid a sudden large influx of \code{put} calls.
% Indeed, in this final implementation, the \code{ConcurrentCache} class needn't
% override the \code{get} method of \code{ConcurrentHashMap}.



\begin{comment}
\subsection{Caches: Amortizing the Cost of a Query or Method Call}

\subsection{Pools: Gating Access to a Constrained Resource}

If the data stored in a data structure is frequently and expensively recomputed
or refetched, and the data values are the same every time, then it is worthwhile
to cache the computation or data fetch. The expense of re-fetching data from
external data sources and recomputing the in-memory structure can often be
amortized, at the expense of stretching the lifetime of these data structures. A
good cache defers the time that an object will be reclaimed, as long as there is
sufficient space to handle the flux of temporary objects your application
creates.

\section{Resource Pools}
\label{sec:resource-pools}
\index{Resource Pool}

\index{Amortizing Costs}
A cache can amortize the cost, in time, of fetching or otherwise initializing the
data stored in an object. A sharing pool can amortize the cost, in space, of
storing the same data in many separate objects. In both cases, the data is the
important part of what is stored.

\marginpar{A \textbf{Resource Pool} is a set of interchangeable storage or
external connections that are expensive to construct.} There is a third case,
where one needs to amortize the cost of the allocations, rather than the cost of
initializing or fetching the data that is stored in this object. A resource pool
stores the result of the allocation, not the data. Therefore, the elements of a
resource pool are interchangeable, because it is the storage, not the values that
matter. It is important to note that, though the data values are not the
important part, the elements of the pool are objects, and are thus intended to
store data! A resource pool handles the interesting case where the data is
temporary, but you need, for performance reasons, the objects to live across many
uses. The protocol for using a resource pool then involves reservation, a period
of private use of the fields of the reserved object, followed by a return of that
object to the pool.

Resource pooling only makes sense if the allocations themselves are expensive.
There are several reasons why a Java object can be expensive to allocate.
Creating and zeroing a large array\index{Large Arrays} in each iteration of a
loop can bog down performance. Creating a new key object to determine whether an
value exists in a map can sometimes contribute a great deal to the load of
temporary objects.

\index{Connection Pools}
A more important example of the need for amortizing the time cost of allocation
comes when this Java object is a proxy for resources outside of Java. If your
application accesses a relational database through the JDBC\index{JDBC}
interface, you will experience the need for resource pooling. There are two kinds
of objects that serve as proxies for resources involving database access. First
are the connections to the database. In most operating systems, establishing a
network connection is an expensive proposition. It also involves reservation of
resoures in the database process. Second are the precompiled SQL statements that
your application uses. As with the connections, these involve setup cost, of the
compilation itself, as well as the reservation of memory resources, that the
database uses to cache certain information about the query.
\end{comment}

\section{Patterns Based on Thread-local Storage}
% %ODO 20110601 stores or stoage? also in bulk storage/stores 

To avoid synchronization, you often need to replicate data structures. \Tls is a
built-in mechanism that Java provides to facilitate this kind of cloning across
threads. 

\begin{wrapfigure}{r}{0.68\textwidth}
\centering
\begin{framedlisting}
class ThreadLocalRandom {
  ThreadLocal<SecureRandom> rng = new ThreadLocal<SecureRandom>() {
    SecureRandom initialValue() {
      SecureRandom random = new SecureRandom();
      random.setSeed(...);
      return random;
    }
  }
   
  public int next() {
    // need 32 bits of data
    return rng.get().next(32);
  }
}
\end{framedlisting}
\end{wrapfigure}
Consider an example of generating random numbers, using the \class{SecureRandom}
class. Instances of this class provide a stream of pseudo-random numbers, in a way that is
cryptographically strong. If you have a singleton instance of this class, you
may experience scalability problems due to lock contention; the contention is
hidden within the \class{SecureRandom} implementation. You can use \tls to avoid
this contention, at the (in this case, small) expense of having one instance per
worker thread. \javaseven adds a \class{ThreadLocalRandom} implementation to the
standard library.


%class Formatter {
  %ThreadLocal<DateFormat> f = new ThreadLocal<DateFormat> {
    %@Override
    %DateFormat initialValue() {
      %return DateFormat.getInstance();
    %}
  %};
%   
  %String format(Date d) {
    %return f.get().format(d);
  %}
%}

\paragraph{\Tls: Reentrancy Gotchas}
A thread-local variable allows you to store one copy of a data structure per
thread. If your code is structured so that one method invokes another, and the
both need to make use of the thread-local variable, you should be cautious of
reentrancy problems. A reentrancy problem is like a race condition, but within
one thread. If first one invocation begins to modify the thread-local variable,
and, before it is done with its modifications, calls another method that begins
its \emph{own} modifications, then you will see unexpected results.

One way to protect against the potential for this problem is to adopt a
\emph{fail-fast} approach. When you grab an entry from thread-local storage for
modification, then assign the thread-local variable to \code{null} for the
duration of the modifications. That way, any reentrancy problems will be exposed
as quickly as possible, as a \code{NullPointerException}. Here, you will get an
exception thrown at the very point of the bug, rather than further down the
line, as is often the case with race and reentrancy bugs.

\paragraph{\Tls: Memory Drag Gotchas}

When using \tls, you need to be cautious of memory drag. Data stored in \tls
will, unless you take some action, live as long as the thread. If you store a
large structure in the local storage of a thread that lasts for the duration of
the program, but only use the structure early in your program's execution, then
that structure will be needlessly consuming space. 

To avoid drag you can explicitly overwrite the entry, by calling {\tt
rng.put(null)}. This must be done by the thread itself, since there is no way
for one thread to access another thread's local storage. You can put this call
at the end of a phase in your program, or when the program terminates. If you
are using the \class{java.util.concurrent} thread pool framework, then you can
use its hooks that are called after a task, or after a thread, terminates. You
can do so by extending the \class{ThreadPoolExecutor} and overriding the
\code{afterExecute} and \code{terminated} methods, respectively.

Another option is to have your thread-local variable store a soft reference to
the data. If you adopt this design, then each thread's copy of the random number
generator will be cleaned up by the garbage collector if it ever needs the
space, and the generator hasn't been used in a while.
\begin{shortlisting}
ThreadLocal<SoftReference<SecureRandom>> rng;
\end{shortlisting}
This design does not clean up every last bit of memory associated with a
thread's entry. Each thread-local variable is a map, from thread to the value,
and so the per-entry objects of the map will stick around. Still, at least you
will not be keeping around a potentially very large structure. You also need to
take care to handle the case when the \jre does get around to clipping the soft
reference, and you come back to generate more random numbers. Still,
this hybrid approach, of combining soft references and \tls, can be quite powerful.

 



\Tls, like the \class{WeakHashMap}, is an example of the \jre hiding some of the
complexity of managing weak references. Under the covers, the \tls implementation
uses weak references so that, if a \class{ThreadLocal} object is reclaimed, then
the storage associated with it, for all threads, will be reclaimed, too. In some
implementations, this will not happen immediately, because these implementations
do not use reference queues. They use an alternative approach that at least
keeps the amount of memory spent on stale \tls bounded.

\begin{comment}
\section{}

\subsection{Avoiding Leaks When Optimizing for Time}
Sometimes a data structure can have multiple lifetime requirements. 
Care is required to make sure that we satisfy all of the requirements.  In this
section we look at two cases where we are balancing the need to save both space and recomputation
time.

\begin{example}{Session State}
\end{example}

\begin{example}{A Caching Sharing Pool}
\end{example}
\end{comment}

\section{Summary}

\begin{itemize}
  \item Soft references can be a useful tool to bound the size of caches, but
  you should not depend solely on it. Some \jres do not implement an LRU
  heuristic at all. Others implement one that may give you unexpected
  evictions, despite there being plenty of free Java heap.
  \item Implementing a concurrent cache is surprisingly difficult!
  \item Thread-local storage is an easy way to keep around clones of data, in
  order to avoid lock contention.
\end{itemize}




