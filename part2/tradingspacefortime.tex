\chapter{Trading Space for Time: Caches, Resource Pools and Thread-Local Stores}
\label{chapter:trading-space-for-time}

There are three important cases of time-space tradeoffs. The first covers
the situation where recomputing attributes, rather than storing them, is a better
choice. The next two cover situations where spending memory to extend the
lifetime of certain objects saves sufficient time to be worthwhile: caches and
resource pools.

\section{Tools: Soft References}
\index{Soft References}

The constructor for a \class{SoftReference} also takes an object as a parameter.
The object lives as it normally would until the point in time when there are no
other strong references to the object. When an object is only softly or weakly
referenced, then it enters a special transitionary lifetime state. The \jre will
keep this object around, for as long as there is enough free Java heap. Once heap
grows tight, the \jre will begin treating the soft references as if they were
weak references --- the soft references that the \jre choses to discard will no
longer inhibit the reclaimability of the softly referenced objects. This
discarding of soft references is sometimes referred to as ``clearing'' soft
references.

The Java language specification makes no specific requirements as to how \jre
implementations should chose which soft references to discard. The only
requirement imposed by the language specification is that the \jre must have
cleared \emph{all} soft references before it throws an
\class{OutOfMemoryException}. That is, it must have reclaimed all objects that
are uniquely owned by soft (or both soft and weak) references before it gives up,
and fails due to heap exhaustion. Early \jres tended to make poor decisions, when
choosing how to clear soft references. One \jre would wait until the heap was
exhausted, at which point it would clear all soft references. Most \javafive and
\javasix \jres use a more sophisticated least recently used (LRU) heuristic.
They keep track of the last time \code{get} was called, on a per-\class{SoftReference}
basis, and begin to clear soft references if their last use was long ago.
Sometimes, they measure this distance relative to the rate of object allocation;
this modified heuristic will not clear soft references if your application isn't
allocating objects at a high rate.

For those \jres that use some sort of LRU heuristic, soft references can form the
basis for implementing caches. You must be careful not to depend on this
heuristic blindly. You should first run an experiment against the \jre to which
you intend to deploy: implement a simple cache with soft references (see
\autoref{chapter:time-space-tradeoffs}); enable verbose garbage collection, and
observe the messages that indicate soft references being cleared. Making this
observation is easier on an IBM than a Sun JVM. To do so on an IBM JVM, enable
verbose garbage collection statistics (by adding \code{-verbose:gc} to the
command line), and track lines of output of this form:
\begin{shortlisting}
<refs soft="27801" weak="3" phantom="0" dynamicSoftReferenceThreshold="19" maxSoftReferenceThreshold="32" />
\end{shortlisting}
It is the number of soft references you need to track. With an
LRU-based soft reference clearing heuristic, you should observe that clearing
occurs at a constant rate. If you observe lulls and spikes in clearing, then you
must not depend on soft references for implementing your caches!



\callout{soft-reference-rule}{Soft Reference Rule}{
Soft references must always be over values, not keys. Otherwise, testing
equality of keys will trigger a use of the reference. This will extend the
lifetime of the value, even though the only use of the entry was in checking to
see if its key matches another.	}

\section{Caches}

If the data stored in a data structure is frequently and expensively recomputed
or refetched, and the data values are the same every time, then it is worthwhile
to cache the computation or data fetch. The expense of re-fetching data from
external data sources and recomputing the in-memory structure can often be
amortized, at the expense of stretching the lifetime of these data structures. A
good cache defers the time that an object will be reclaimed, as long as there is
sufficient space to handle the flux of temporary objects your application
creates.

\subsection{Implementing a Concurrent Cache}
\label{sec:lifetime-management-concurrency-issues}

If your program operates with many concurrent threads, you have to program
differently, because straightforward implementations of the above strategies will
result in concurrency issues. One of the primary problems will be lock
contention, as threads concurrently poll a reference queue. An important example
of this problem shows up in the implementation of a cache that can support many
concurrent users.\index{Caches, Concurrency Issues}

A cache is a map, usually of bounded size, with an eviction strategy for
maintaining that bound.\index{Caches} The Java standard library provides a
concurrent map implementation, in the form of the \class{ConcurrentHashMap}
class, but this is not a cache, because it has no eviction hooks with which one
can bound its size.

Following the soft reference rule, and using the basic guidelines for managing
reference queues from \autoref{sec:reference-queue-basics}, leads to a first
attempt at a \class{ConcurrentCache} implementation. You can extend the basic
concurrent hashmap, wrapping the map's values with soft references:
\begin{shortlisting}
class ConcurrentCache<K,V> extends ConcurrentHashMap<K,SoftReference<V>> {
   private final ReferenceQueue<V> refQueue = new ReferenceQueue<V>();
   
   protected void cleanupQueue() {
      SoftReference<V> v;
      while( (v = refQueue.poll()) != null) {
         remove(???); // oops!
      }
   }
}
\end{shortlisting}
Oops! This implementation provides no way to remove the key from the map, when
cleaning up the evicted entries. To fix this, you'll need to stash a pointer to
the key in the soft reference wrapper. It would be nice if the
\class{ConcurrentHashMap} implementation let you extend
its implementation so that its \class{\$Entry} class extended soft reference;
the \class{\$Entry} would serve this role perfectly. Instead, you have to
replicate this pointer structure, at a silly but unavoidable cost of memory
bloat. You can do so in a \class{CacheSoftReference} wrapper:
\begin{shortlisting}
class CacheSoftReference<K, V> extends SoftReference<V> {
   private final K k;
   
   public CacheSoftReference(K k, V v, ReferenceQueue<V> refQueue) {
      super(v, refQueue);
      this.k = k;
   }
}

class ConcurrentCache<K,V> extends ConcurrentHashMap<K,CacheSoftReference<K,V>> {
   private final ReferenceQueue<V> refQueue = new ReferenceQueue<V>();
   
   protected void cleanupQueue() {
      SoftReference<V> v;
      // poll causes lock contention!
      while( (v = refQueue.poll()) != null) {
         remove(v.k);
      }
   }
   
   public V put(K key, V value) {
      cleanupQueue();
      return super.put(key, new CacheSoftReference<K,V>(key,value,refQueue));
   }
   
   public V get(K key) {
      cleanupQueue();
      return super.get(key);
   }
}
\end{shortlisting}
This implementation still suffers from several critical problems. First, every
call to \code{put} must check the reference queue for pending evictions in order
to avoid unbounded growth of the eviction queue --- in the steady state,
\code{put} calls are likely to cause evictions. Even though \code{get} calls
won't cause evictions, in order to avoid pending evictions piling up as cached
elements are discovered to be unused, every call to \code{get} must also poll for
evictions. This can result in foiling the concurrency aspect of the
\class{ConcurrentHashMap}. Second, if the cache as a whole goes unused for a long
period of time, the pending evictions will pile up.

The only way to fix the lock contention problem, at least as of \javasix, is to
spawn a thread that periodically polls the reference queue for evicted entries.
This will also fix the second problem. This spawned thread's \code{run} method
will look just like the \code{cleanupQueue} method above, except that it should
loop indefinitely, and call \code{refQueue.remove()} rather than \code{poll()};
the former blocks until an eviction occurs (though you must still be careful to
check the return value for \code{null}, despite what the Javadocs claim).

At first sight, it would seem that you should be able to remove the calls to
\code{cleanupQueue} from the \code{put} and \code{get} methods. This, after all,
was the whole point of introducing the cleanup thread. However, this modified
implementation, while an improvement, suffers from a new problem. Now, if you
remove the \code{cleanupQueue} calls, when there is a large spike of \code{put}
calls in a short period of time, you are at risk of running out of Java heap due
to a large pileup of pending evictions.

You must have a safety valve in place to prevent this situation. One possibility
is to keep an approximate count of the number of \code{put} calls, and call
\code{cleanupQueue} only periodically. In order to avoid lock contention in
maintaining this count, you can do so in an unsynchronized way. There is still a
pathologic possibility that every racey increment of the put counter won't
actually increment the counter. If this worries you, you can use an
\class{AtomicInteger}, at increased expense. Instead of calling
\code{cleanupQueue} directly, the \code{put} method now calls a new
\code{helpCleaner} method:
\begin{shortlisting}
   static private final int SAFETY_VALVE = 1000;
   private void helpCleaner() {
      if (putCount.incrementAndGet() >= SAFETY_VALVE) {
         putCount.set(0);
         cleanupQueue();
      }
   }
\end{shortlisting}
There is no reason for \code{get} calls to call this method. The only point of
this safety valve is to avoid a sudden large influx of \code{put} calls. Indeed,
in this final implementation, the \code{ConcurrentCache} class needn't override
the \code{get} method of \code{ConcurrentHashMap}.



\section{Resource Pools}
\label{sec:resource-pools}
\index{Resource Pool}

\index{Amortizing Costs}
A cache can amortize the cost, in time, of fetching or otherwise initializing the
data stored in an object. A sharing pool can amortize the cost, in space, of
storing the same data in many separate objects. In both cases, the data is the
important part of what is stored.

\marginpar{A \textbf{Resource Pool} is a set of interchangeable storage or
external connections that are expensive to construct.} There is a third case,
where one needs to amortize the cost of the allocations, rather than the cost of
initializing or fetching the data that is stored in this object. A resource pool
stores the result of the allocation, not the data. Therefore, the elements of a
resource pool are interchangeable, because it is the storage, not the values that
matter. It is important to note that, though the data values are not the
important part, the elements of the pool are objects, and are thus intended to
store data! A resource pool handles the interesting case where the data is
temporary, but you need, for performance reasons, the objects to live across many
uses. The protocol for using a resource pool then involves reservation, a period
of private use of the fields of the reserved object, followed by a return of that
object to the pool.

Resource pooling only makes sense if the allocations themselves are expensive.
There are several reasons why a Java object can be expensive to allocate.
Creating and zeroing a large array\index{Large Arrays} in each iteration of a
loop can bog down performance. Creating a new key object to determine whether an
value exists in a map can sometimes contribute a great deal to the load of
temporary objects.

\index{Connection Pools}
A more important example of the need for amortizing the time cost of allocation
comes when this Java object is a proxy for resources outside of Java. If your
application accesses a relational database through the JDBC\index{JDBC}
interface, you will experience the need for resource pooling. There are two kinds
of objects that serve as proxies for resources involving database access. First
are the connections to the database. In most operating systems, establishing a
network connection is an expensive proposition. It also involves reservation of
resoures in the database process. Second are the precompiled SQL statements that
your application uses. As with the connections, these involve setup cost, of the
compilation itself, as well as the reservation of memory resources, that the
database uses to cache certain information about the query.

\section{Avoiding Leaks When Optimizing for Time}
Sometimes a data structure can have multiple lifetime requirements. 
Care is required to make sure that we satisfy all of the requirements.  In this
section we look at two cases where we are balancing the need to save both space and recomputation
time.

\begin{example}{Session State}
\end{example}

\begin{example}{A Caching Sharing Pool}
\end{example}

\section{Avoiding Contention: Thread-local Stores}
% TODO 20110601 no reetrant safety given by thread-local storage
% %ODO 20110601 stores or stoage? also in bulk storage/stores 

The last advanced memory management feature offered by Java is the ability to
associate memory with a thread. \Tls provides a way to avoid
synchronization costs, often at the expense of some degree of wasted memory. To
avoid synchronization, you often need to replicate some data structures. This
feature is, of course, only helpful if your program runs with multiple threads.

Consider an example of using the \class{SecureRandom} class. Instances of this
class provide a stream of pseudo-random numbers, in a way that is
cryptographically strong. If you have a singleton instance of this class, you may
experience scalability problems due to lock contention; the contention is hidden
within the \class{SecureRandom} implementation. You can use \tls
to avoid this contention, at the (in this case, small) expense of having one
instance per worker thread:
\begin{shortlisting}
class MyRandomNumberGenerator {
   static ThreadLocal<SecureRandom> rng = new ThreadLocal<SecureRandom>() {
      protected SecureRandom initialValue() {
         SecureRandom random = new SecureRandom();
         random.setSeed(/*some good seed*/);
         return random;
      }
   }
   
   public int next() {
      return rng.get().next(32); // need 32 bits of data
   }
}
\end{shortlisting} 
\javaseven adds a \class{ThreadLocalRandom} implementation to the standard
library.

Data stored in \tls will live as long as the thread. If you need
the memory to be reclaimed before the thread terminates, you must explicitly set
the storage to \code{null} via a call to \code{rng.set(null)}. If you are using
the \class{java.util.concurrent} thread pool framework, then you can use its
hooks that are called after a task, or after a thread, terminates. You can do so
by extending the \class{ThreadPoolExecutor} and overriding the
\code{afterExecute} and \code{terminated} methods, respectively.

\Tls, like the \class{WeakHashMap}, is an example of the \jre hiding some of the
complexity of managing weak references. Under the covers, the \tls implementation
uses weak references so that, if a \class{ThreadLocal} object is reclaimed, then
the storage associated with it, for all threads, will be reclaimed, too. In some
implementations, this will not happen immediately, because these implementations
do not use reference queues. They use an alternative approach that at least
keeps the amount of memory spent on stale \tls bounded.

\section{Summary}






