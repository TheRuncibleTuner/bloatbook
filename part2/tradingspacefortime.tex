\chapter[Trading Space for Time]{Trading Space for Time: Caches, Resource Pools and Thread-Local
Stores}
\label{chapter:trading-space-for-time}

There are three important cases of time-space tradeoffs. The first covers
the situation where recomputing attributes, rather than storing them, is a better
choice. The next two cover situations where spending memory to extend the
lifetime of certain objects saves sufficient time to be worthwhile: caches and
resource pools.

\section{Patterns Based on Soft References}
\index{Soft References}

The constructor for a \class{SoftReference} also takes an object as a parameter.
The object lives as it normally would until the point in time when there are no
other strong references to the object. When an object is only softly or weakly
referenced, then it enters a special transitionary lifetime state. The \jre will
keep this object around, for as long as there is enough free Java heap. Once heap
grows tight, the \jre will begin treating the soft references as if they were
weak references --- the soft references that the \jre choses to discard will no
longer inhibit the reclaimability of the softly referenced objects. This
discarding of soft references is sometimes referred to as ``clearing'' soft
references.

The Java language specification makes no specific requirements as to how \jre
implementations should chose which soft references to discard. The only
requirement imposed by the language specification is that the \jre must have
cleared \emph{all} soft references before it throws an
\class{OutOfMemoryException}. That is, it must have reclaimed all objects that
are uniquely owned by soft (or both soft and weak) references before it gives up,
and fails due to heap exhaustion. Early \jres tended to make poor decisions, when
choosing how to clear soft references. One \jre would wait until the heap was
exhausted, at which point it would clear all soft references. Most \javafive and
\javasix \jres use a more sophisticated least recently used (LRU) heuristic.
They keep track of the last time \code{get} was called, on a per-\class{SoftReference}
basis, and begin to clear soft references if their last use was long ago.
Sometimes, they measure this distance relative to the rate of object allocation;
this modified heuristic will not clear soft references if your application isn't
allocating objects at a high rate.

For those \jres that use some sort of LRU heuristic, soft references can form the
basis for implementing caches. You must be careful not to depend on this
heuristic blindly. You should first run an experiment against the \jre to which
you intend to deploy: implement a simple cache with soft references (see
\autoref{chapter:time-space-tradeoffs}); enable verbose garbage collection, and
observe the messages that indicate soft references being cleared. Making this
observation is easier on an IBM than a Sun JVM. To do so on an IBM JVM, enable
verbose garbage collection statistics (by adding \code{-verbose:gc} to the
command line), and track lines of output of this form:
\begin{shortlisting}
<refs soft="27801" weak="3" phantom="0" dynamicSoftReferenceThreshold="19" maxSoftReferenceThreshold="32" />
\end{shortlisting}
It is the number of soft references you need to track. With an
LRU-based soft reference clearing heuristic, you should observe that clearing
occurs at a constant rate. If you observe lulls and spikes in clearing, then you
must not depend on soft references for implementing your caches!



\callout{soft-reference-rule}{Soft Reference Rule}{
Soft references must always be over values, not keys. Otherwise, testing
equality of keys will trigger a use of the reference. This will extend the
lifetime of the value, even though the only use of the entry was in checking to
see if its key matches another.	}

\subsection{Caching a Subset of a Full Data Set}

\subsection{Caching the Output of an Expensive Operation}

\subsection{Gating Access to a Constrained Resource}

If the data stored in a data structure is frequently and expensively recomputed
or refetched, and the data values are the same every time, then it is worthwhile
to cache the computation or data fetch. The expense of re-fetching data from
external data sources and recomputing the in-memory structure can often be
amortized, at the expense of stretching the lifetime of these data structures. A
good cache defers the time that an object will be reclaimed, as long as there is
sufficient space to handle the flux of temporary objects your application
creates.

\subsection{Example: Implementing a Concurrent Cache}
\label{sec:lifetime-management-concurrency-issues}

A cache is a map, usually of bounded size, with an eviction strategy for
maintaining that bound.\index{Caches} The Java standard library provides a
concurrent map implementation, in the form of the \class{ConcurrentHashMap}
class, but this is not a cache, because it has no eviction hooks with which one
can bound its size.

\begin{wrapfigure}[9]{r}{0.34\textwidth}
\centering
\begin{framedlisting}
new MapMaker().concurrencyLevel(4).softKeys().maximumSize(1000).createMap();
\end{framedlisting}
\caption{Using Google's Guava library to create a concurrent cache.}
\label{fig:mapmaker-make-concurrent-cache}
\end{wrapfigure}
The Java standard library does not provide a concurrent cache implementation.
Thankfully, there are other libraries at your disposal, such as \class{MapMaker}
from the Google Guava libraries~\cite{google-guava}. With \class{MapMaker} you can create
a concurrent cache by calling the sequence in
\autoref{fig:mapmaker-make-concurrent-cache}. It is nonetheless useful to step
through what it takes to make this work. In doing so, you will learn about
managing soft references and reference queues.
% One of the primary problems you
%will run into is lock contention, as threads concurrently poll a reference
%queue.
\index{Concurrency}\index{Caches}

\begin{wrapfigure}{r}{0.61\textwidth}
\centering
\begin{figurelisting}
class Cache<K,V> {
  ConcurrentHashMap<K,SoftReference<V>> m;
  ReferenceQueue<V> q;
   
  void put(K k, V v) {
    cleanupQueue();
    m.put(k, new SoftReference(v, q));
  }
  
  void cleanupQueue() {
    Reference r;
    while((r = q.poll()) != null)
       m.remove(???); // oops!
  }
}
\end{figurelisting}
\caption{A first attempt at a concurrent cache.}
\label{fig:concurrent-cache-first-attempt}
\end{wrapfigure}

\paragraph{Eviction Mechanisms}
To implement the eviction aspect of a cache, you can leverage soft references.
You need to follow the soft reference rule from earlier in this chapter: softly
reference the values, never the keys. You can extend the basic
\class{ConcurrentHashMap}, wrapping the map's values with soft references. You
also need to follow the reference queue polling rule from
\autoref{reference-queue-rule}: poll the queue at least as often as soft
references are created --- in the steady state, \code{put} calls are likely to
cause evictions. This leads to the first attempt at a concurrent cache
implementation, shown in \autoref{fig:concurrent-cache-first-attempt}. The first
thing you will discover is that your cache implementation cannot inherit from
the concurrent hash map with softly reference values, because then the
\code{put} and \code{get} methods would have to expose the soft references.
Instead, your cache must delegate to the underlying map. 

\begin{wrapfigure}{l}{0.43\textwidth}
\centering
\begin{framedlisting}
class CacheValue<K, V> extends SoftReference<V> {
  K key;
   
  CacheValue(K k, V v, ReferenceQueue<V> q) {
    super(v, q);
    this.key = k;
  }
}
\end{framedlisting}
\caption{You will need a special value reference that keeps a reference to the
key, to allow you to remove the entry when it is evicted from the cache.}
\end{wrapfigure}

Unfortunately, this doesn't quite work. The variable \code{v} references the
value, not the key. Therefore, this first implementation provides no way to
remove evicted value from the map. To fix this, you'll need to stash, in the a
subclass of the soft reference wrapper, a pointer to the key.\footnote{It would
be nice if the \class{ConcurrentHashMap} implementation let you extend its
implementation so that its \class{\$Entry} class extended soft reference; the
\class{\$Entry} would serve this role perfectly.} Then, you can update the
\class{Cache} implementation to extend
\class{ConcurrentHashMap<K, CacheValue<K,V>>}, as shown in
\autoref{fig:concurrent-cache-second-attempt}.

\begin{wrapfigure}{l}{0.61\textwidth}
\centering
\begin{figurelisting}
class Cache<K,V> {
  ConcurrentHashMap<K,CacheValue<K,V>> m;
  ReferenceQueue<V> q;
   
  void cleanupQueue() {
    Reference r;
    while( (r = q.poll()) != null)
       remove(((CacheValue)r).k);
  }
   
  void put(K k, V v) {
    cleanupQueue();
    m.put(key,new CacheValue<K,V>(k,v,q));
  }
}
\end{figurelisting}
  %public V get(K k) {
    %cleanupQueue();
    %return super.get(k);
%  }
\caption{A second attempt at a concurrent cache.}
\label{fig:concurrent-cache-second-attempt}
\end{wrapfigure}

\paragraph{The Need for a Cleanup Thread}
This updated implementation still suffers from two critical problems. First, the
reference queue \code{poll} method is internally synchronized. Every call to
\code{put} and \code{get} must poll the reference queue at least once, to avoid
unbounded pile-up in the queue. This can result in severe lock contention, which
pretty much foils the concurrency aspect of the \class{ConcurrentHashMap}.
Also, if the cache as a whole goes unused for a long period of time, then the
objects already queued up in the reference queue will suffer from memory drag
(see \autoref{drag}). These queued up objects will never be polled, since
neither \code{put} nor \code{get} will be called. The good news is that this
won't result in a memory leak, where objects pile up without bound, but it is
still a potential source of memory problems. You may find that you are wasting
quite a bit of memory in the reference queues of quiescent caches.

One way to fix the lock contention problem is to have a separate thread be
responsible for polling the reference queue. This will also fix the second
problem, as long as the polling thread is active for as long as the cache itself
is around. This spawned thread's \code{run} method will look just like the
\code{cleanupQueue} method above, except that it should loop indefinitely.

\paragraph{Avoiding Spikes}
At first sight, it would seem that you should be able to remove the calls to
\code{cleanupQueue} from the \code{put} and \code{get} methods. The cleanup
thread should take care of all necessary polling, right? However, when there is
a large spike of \code{put} calls in a short period of time, you are at risk of
running out of Java heap due to a large pileup of pending evictions.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\begin{framedlisting}
AtomicInteger putCount;
void cleanupQueueHelper() {
  if (putCount.incrementAndGet() >= 1000) {
    putCount.set(0);
    cleanupQueue();
  }
}
\end{framedlisting}
\caption{To avoid spikes, \code{put} must call this method.}
\label{fig:new-cleanup-queue}
\end{wrapfigure}
You must have a safety valve in place to prevent this situation. One possibility
is to keep an approximate count of the number of \code{put} calls, and call
\code{cleanupQueue} only periodically. In order to avoid lock contention in
maintaining this count, you can do so in an unsynchronized way. There is still a
pathologic possibility that every racey increment of the put counter won't
actually increment the counter. If this worries you, you can use an
\class{AtomicInteger}, at increased expense. A new \code{cleanupQueueHelper}
method is shown in \autoref{fig:new-cleanup-queue}, and \code{poll} now calls
this method. There is no reason for \code{get} calls to call this method. The
only point of this safety valve is to avoid a sudden large influx of \code{put} calls.
% Indeed, in this final implementation, the \code{ConcurrentCache} class needn't
% override the \code{get} method of \code{ConcurrentHashMap}.



\section{Resource Pools}
\label{sec:resource-pools}
\index{Resource Pool}

\index{Amortizing Costs}
A cache can amortize the cost, in time, of fetching or otherwise initializing the
data stored in an object. A sharing pool can amortize the cost, in space, of
storing the same data in many separate objects. In both cases, the data is the
important part of what is stored.

\marginpar{A \textbf{Resource Pool} is a set of interchangeable storage or
external connections that are expensive to construct.} There is a third case,
where one needs to amortize the cost of the allocations, rather than the cost of
initializing or fetching the data that is stored in this object. A resource pool
stores the result of the allocation, not the data. Therefore, the elements of a
resource pool are interchangeable, because it is the storage, not the values that
matter. It is important to note that, though the data values are not the
important part, the elements of the pool are objects, and are thus intended to
store data! A resource pool handles the interesting case where the data is
temporary, but you need, for performance reasons, the objects to live across many
uses. The protocol for using a resource pool then involves reservation, a period
of private use of the fields of the reserved object, followed by a return of that
object to the pool.

Resource pooling only makes sense if the allocations themselves are expensive.
There are several reasons why a Java object can be expensive to allocate.
Creating and zeroing a large array\index{Large Arrays} in each iteration of a
loop can bog down performance. Creating a new key object to determine whether an
value exists in a map can sometimes contribute a great deal to the load of
temporary objects.

\index{Connection Pools}
A more important example of the need for amortizing the time cost of allocation
comes when this Java object is a proxy for resources outside of Java. If your
application accesses a relational database through the JDBC\index{JDBC}
interface, you will experience the need for resource pooling. There are two kinds
of objects that serve as proxies for resources involving database access. First
are the connections to the database. In most operating systems, establishing a
network connection is an expensive proposition. It also involves reservation of
resoures in the database process. Second are the precompiled SQL statements that
your application uses. As with the connections, these involve setup cost, of the
compilation itself, as well as the reservation of memory resources, that the
database uses to cache certain information about the query.

\section{Patterns Based on Thread-local Storage}
% TODO 20110601 no reetrant safety given by thread-local storage
% %ODO 20110601 stores or stoage? also in bulk storage/stores 

The last advanced memory management feature offered by Java is the ability to
associate memory with a thread. \Tls provides a way to avoid
synchronization costs, often at the expense of some degree of wasted memory. To
avoid synchronization, you often need to replicate some data structures. This
feature is, of course, only helpful if your program runs with multiple threads.

Consider an example of using the \class{SecureRandom} class. Instances of this
class provide a stream of pseudo-random numbers, in a way that is
cryptographically strong. If you have a singleton instance of this class, you may
experience scalability problems due to lock contention; the contention is hidden
within the \class{SecureRandom} implementation. You can use \tls
to avoid this contention, at the (in this case, small) expense of having one
instance per worker thread:
\begin{shortlisting}
class MyRandomNumberGenerator {
   static ThreadLocal<SecureRandom> rng = new ThreadLocal<SecureRandom>() {
      protected SecureRandom initialValue() {
         SecureRandom random = new SecureRandom();
         random.setSeed(/*some good seed*/);
         return random;
      }
   }
   
   public int next() {
      return rng.get().next(32); // need 32 bits of data
   }
}
\end{shortlisting} 
\javaseven adds a \class{ThreadLocalRandom} implementation to the standard
library.

Data stored in \tls will live as long as the thread. If you need
the memory to be reclaimed before the thread terminates, you must explicitly set
the storage to \code{null} via a call to \code{rng.set(null)}. If you are using
the \class{java.util.concurrent} thread pool framework, then you can use its
hooks that are called after a task, or after a thread, terminates. You can do so
by extending the \class{ThreadPoolExecutor} and overriding the
\code{afterExecute} and \code{terminated} methods, respectively.

\Tls, like the \class{WeakHashMap}, is an example of the \jre hiding some of the
complexity of managing weak references. Under the covers, the \tls implementation
uses weak references so that, if a \class{ThreadLocal} object is reclaimed, then
the storage associated with it, for all threads, will be reclaimed, too. In some
implementations, this will not happen immediately, because these implementations
do not use reference queues. They use an alternative approach that at least
keeps the amount of memory spent on stale \tls bounded.

\section{Avoiding Leaks When Optimizing for Time}
Sometimes a data structure can have multiple lifetime requirements. 
Care is required to make sure that we satisfy all of the requirements.  In this
section we look at two cases where we are balancing the need to save both space and recomputation
time.

\begin{example}{Session State}
\end{example}

\begin{example}{A Caching Sharing Pool}
\end{example}


\section{Summary}






