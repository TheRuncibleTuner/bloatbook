
\chapter{Memory Health}
\label{chapter:memory-health}

A data structure may be big, but how do you know if it is too big? This chapter
introduces the concept of memory health, a way to gauge how efficiently a data
structure uses memory. Java developers often have to piece together many smaller
objects to build a single data structure. Memory health is a way to understand
the overhead introduced in each object, the impact of connecting these objects
into larger data structures, and, finally, how a data structure scales as the
amount of data it holds grows. Understanding memory health can provide valuable
insight into each step of the data modeling process.

Later chapters show how to estimate both the size and health of data structure
designs. Both of these are important. Memory health is independent of size: a
small structure can be unhealthy, and a large structure can be healthy. A
healthy data structure will generally scale well, while unhealthy data
structures are often the root causes of memory footprint problems.

%The examples .

\section{Distinguishing Data from Overhead}
\label{sec:bloat-def}

The size of a data structure is the number of bytes it occupies. The
\emph{health} of a data structure tells you what these bytes are accomplishing.
Is the memory being spent on something important?  A very useful way to classify
bytes is as either data or overhead. Bytes classified as data represent the real
data intrinsic to your program. All other bytes are overhead of the particular
representation you have chosen for that data. A healthy data structure, from a
space standpoint, is one with a low overhead.  Sometimes there are tradeoffs; a
larger design may be necessary for performance. Sometimes, though, choosing a
design with lower memory overhead actually improves performance.

In Java, it is all too easy to design a data structure that is bloated by a
factor of more than 50\%; this means that over half of the data structure is
devoted to things unrelated to the data that the application cares about. To
understand why there is so much overhead, it is useful to dig deeper into its
different sources:
\begin{itemize}
\item \emph{JVM overhead.} \index{Object Headers} \index{JVM Overhead} Each
object has a header imposed by the JVM to facilitate various administrative
tasks, such as garbage collection. In addition, the JVM may impose additional
object alignment overhead.
\item \emph{Pointers}. Pointers glue objects together into data structures.
Pointers can be null or non-null.
\item \emph{Bookkeeping fields}. \index{Bookkeeping fields} Not all primitive
fields store real application data. Some are bookkeeping fields needed by the
representation.
\end{itemize}


\callout{callout:memory-bloat-factor}{The Memory Bloat Factor}{
\index{Memory Bloat Factor}
    An important metric for evaluating the health of a data model
    design is the fraction of overhead in the objects of that data
    model. The \emph{memory bloat factor}, or simply \emph{bloat
      factor}, is the ratio of overhead to total size. 
}
% TODO: add alignment
\begin{figure}
  \centering
  \includegraphics[width=.70\textwidth]{part1/Figures/memoryhealth/eight-char-string.pdf}
  %\includegraphics{eight-char-string}
  \caption{An eight-character string in \javasix.}
  \label{fig:eight-char-string}
\end{figure}

\begin{example}{An 8-Character String}
 You learned in the quiz in \autoref{chapter:introduction} that an 8-character
 string occupies 56 bytes, which seems surprisingly high. The actual character data takes up 16
 bytes, since Java supports the international character set, which uses 2-bytes
 per character. So it costs 16 bytes to represent the eight characters
 themselves. The other 40 bytes are pure overhead. This structure has a
 \emph{bloat factor} of 71\%. The actual data occupies only 29\%. These numbers
 vary from one \jre to another, but the overhead is always very high. (Unless
 otherwise noted, all measurements in this book are from the 32-bit \oracle
 \javasix \jre.)

Figure~\ref{fig:eight-char-string} shows why the bloat factor is so high.
Strings have all three kinds of overhead -- JVM overhead, pointers, and
bookkeeping fields. Every Java string is really two objects: a {\tt String} that
serves as a wrapper, and a {\tt char} array with the actual characters. Two
objects means two object headers, plus the pointer glueing the two objects
together. The {\tt String} object is entirely overhead, containing no real data.
Part of its cost is three bookkeeping fields: a length and an offset, which have
meaning only when this string is the result of a substring operation, and a
saved hashcode. Adding all of this together, the overhead is 40 bytes.
\end{example}

If you were to design a string from scratch, 20\% overhead might seem a
reasonable target. For such a common data type, it is important to obtain a low
overhead.
Unfortunately, for a string to have only 20\% overhead with its current
implementation, it would need to be 80 characters long. As bad as this seems,
this string representation does at least have the benefit that it amortizes away
its overhead. The overhead cost is always 40 bytes, so overhead of a string
approaches 0\% for larger and larger strings.  For some data structures it is
not possible to amortize overhead costs, as discussed in
Section~\ref{sec:scalability}.

Strings, in particular short strings, are pervasive. It is important to be aware
of the overhead of strings when incorporating them into your data design. Given
the overhead of Java strings, the choices you make in Java need to be different
than in a language like C, where the string overhead is minimal. Making informed
choices is critical for the overall memory health of an application. There will
be more on strings in later chapters.

\section{Entities and Collections}

A string is an example of a very simple data structure.  When
modeling a more complex data structure, you design classes to
represent application entities, and choose collections in which to
store them. Seemingly small choices you make in both cases can have a
big impact on memory bloat. For example, the choice of the initial
capacity of a collection can make or break the scalability of your
design. 
%The memory health approach looks at entities and collections
%separately, since there are different kinds of choices and pitfalls in
%each.

To help understand the health of more complex data structures, it is useful to
have a diagram notation that spells out the impacts of various choices.  An
\emph{Entity-Collection (EC) diagram} is a cousin of an Entity-Relation (ER)
diagram, that exposes just the right level of implementation
detail to help calculate the memory bloat factor of a complex data structure. 
As its name implies, an EC
diagram shows the entities and collections of a design. It is important to
remember that the entities and collections in the diagram are abstracted from
the actual data structure objects. A diagram box may be hiding other objects
that help implement the entity or collection. For example, a diagram box for a
\code{String} entity represents both the \code{String} object and its
underlying character array.

\callout{callout:ec-diagram}{The Entity-Collection (EC) Diagram}{
\index{Entity-Collection Diagram}

\begin{center}
  \includegraphics[width=0.7\textwidth]{part1/Figures/memoryhealth/content-schematic}
\end{center}

In an EC diagram, there are two types of boxes: pentagons and
rectangles. Each box summarizes the implementation details of a portion
of a data structure.  Pentagons represent collection infrastructure,
and rectangles represent the entities of your application.

An EC diagram represents the {\em containment} structure of your data
model. So, an edge from a collection to a rectangular entity indicates
membership of the entity in the collection. This also means that, if a
certain type of collection or entity is used in multiple ways in the
same data structure, then this type will appear in multiple locations
--- one in each location in the data structure in which that type is
used.

The notation $\times N = M$ inside each node means there are $N$
objects of that type in that location in the data structure, and in
total these objects occupy $M$ bytes of memory. Each edge in an
EC diagram is labeled with
the average fanout from the source node to the target node.
%\caption{--- The Entity-Collection (EC) Diagram}

%Notice how an EC diagram summarizes away some of the details that an
%ER diagram shows, and shows some implementation details than an ER
%diagram does not include.
%The EC diagram represents collection choices, rather than the
%relations they implement.
%where an ER diagram would show a single relation node for an M-to-N
%relation, 
Where an ER or UML diagram would show the attributes of each entity,
but ignore the implementation costs, an EC diagram summarizes the
total cost in the single size number shown in each node. Where these other
diagrams would use an edge to show a relation or role, an EC diagram
uses a node to summarize the collections that implement a relation.
%Unlike UML diagrams, relationships are elevated to be entities, rather
%than edges. The reason is that in a language like Java, implementation
%of the relationship itself is often a big cause of expense, so it is
%important to make that visible. A very common pattern is to implement
%relationships as collections. 
 }

%%%% this is the old Collection of Strings example
%In Figure~\ref{fig:content-schematic}, there are two
%\code{Collection}s which together occupy 200 bytes, and ten
%\code{String}s occupy 880 bytes. There are on average five elements
%contained in each of the two \code{Collection}. Since the overhead
%of a \code{String} is 48 bytes and there are 10 \code{String}s,
%the total overhead is 480 bytes or 55\%. Data occupies 400 bytes, so
%the average \code{String} length is 10. The two \code{Collection}s
%are pure overhead, so the total overhead of this data structure is 680
%bytes, or 63\%.



% The name of an entity is the name of the top object. Entities also
% summarize objects of the same type that are repeated in a data
% structure. The \code{String} entity in
% Figure~\ref{fig:content-schematic} represents the 10 \code{String}s
% in the two \code{Collection}s. The memory cost of an entity is the
% sum over all of its objects. This summarization removes clutter and
% helps understanding.

%\begin{figure}
%  \centering
%  \includegraphics[width=0.9\textwidth]{Figures/content-schematic-relationship}
%  \caption{A content schematic of a map from 10K \code{Strings} to 10K \code{Doubles}.}
 % \label{fig:content-schematic-relationship}
%\end{figure}



%\section{Example: Sorted Map of Doubles}
%\label{section:mapofdoubles}

 \begin{example}{A Monitoring System}
   A monitoring program collects samples from a distributed
   system. Each sample it collects consists of a unique timestamp and a
   value, each represented as a double. The samples arrive out of
   order. The task is to display samples in chronological order, after all of
   the data has been collected. At that point the user may also perform an
   occassional lookup of a sample by timestamp. The
   solution requires a data structure to store the samples.
\end{example}

A map is a convenient way to store these timestamp-value pairs. A regular
\code{HashMap} only solves part of the monitoring system problem. To be able
to display the samples in chronological order, the entries have to be sorted by
timestamp.
The first idea that leaps forward is to use a \code{TreeMap}. A
\code{TreeMap} is a map that maintains its entries in sorted order, so this
appears to be a perfect choice. The only problem is the memory cost.

An EC diagram of a \code{TreeMap} storing 100 samples is shown in
Figure~\ref{fig:content-schematic-treemap-doubles}.  This diagram gives a
schematic view of the \code{TreeMap} and the entities it contains, along with
their costs. The total cost, obtained by adding up the entity and collection
costs, is 6,448 bytes. The real data consumes only 1,600 bytes of this, since a
double occupies 8 bytes and there are 200 of them. Therefore, the bloat factor
is 75\%.

\begin{figure}
  \centering
  %\includegraphics[width=0.4\textwidth]{Figures/treemap-doubles}
  \includegraphics[width=0.7\textwidth]{part1/Figures/memoryhealth/treemap-doubles}
  \caption{EC Diagram for 100 samples stored in a \code{TreeMap}}
  \label{fig:content-schematic-treemap-doubles}
\end{figure} 
 
Looking at the individual parts of the EC diagram can provide some insight into
the source of this overhead. First, the sample timestamps and values are stored
as \code{Double} objects. This is because the standard Java collection APIs take
only \code{Object}s as parameters, not scalars, forcing you to box your scalars
before putting them in a collection. Even with Java's autoboxing, the compiler
still generates a boxed scalar in order to store a value in a standard
collection.  A single instance of a \code{Double} is 16 bytes, so 200
\code{Doubles} occupy 3,200 bytes. Since the data is only 1,600 bytes, 50\% of
the \code{Double} objects is actual data, and 50\% is overhead. This is a high
price for a basic data type.

The \code{TreeMap} infrastructure occupies an additional 3,248 bytes of
memory. All of this is overhead. What is taking up so much space? 
\code{TreeMap}, like other collections in Java, has a wrapper object, the
\code{TreeMap} object itself, along with internal objects that implement
the collection functionality. Each type of collection in the standard library
has a different kind of infrastructure: some are built from arrays, some from
separate entry objects linked together, and some use a combination of both.
Internally, \code{TreeMap} is a self-balancing search tree. It has one
node object for every value stored in the map. Nodes maintain pointers to
parents and siblings, as well as to the keys and values\footnote{There is some
variation among \jres.}.

Using a \code{TreeMap} is not \textit{a priori} a bad design. It depends on
whether the overhead is buying something useful. \code{TreeMap} has a high
memory cost because it maintains its entries in sorted order while they are
being randomly inserted and deleted. It constantly maintains a sorted order. If
you need this capability, for instance, if you have a real time monitor that is
constantly being updated, and you need to look at the data in order, then
\code{TreeMap} is a great data structure.  But if you do not need
this capability, there are other data structures that could provide sorted-map
functionality with less memory consumption. In this example, the data needs to
be sorted only after data collection is complete. There is an initial load phase
followed by a use phase. The sorted order is only needed during the second
phase, after all the data is loaded. This load-and-use behavior is a common
pattern, and it can be exploited to choose a more memory-efficient
representation.

Of course, another nice aspect of \code{TreeMap} is that it can be pulled off
the shelf. It would be ideal if there were another collection that provides just
the needed functionality. If not, maybe there is a way to easily extend another
collection class by adding a thin wrapper around it. Writing your own collection
class from scratch should rarely be necessary, and is not recommended.

\section{Two Memory-Efficient Designs}
\label{sec:better-designs} 

This section describes two other data structure designs to store the monitoring
system samples. These designs use less memory and do not maintain sorted order
while the data is being loaded. This is not a problem, since the data can be
sorted after loading.

The first design stores the samples in an \code{ArrayList}, where each entry
is a \code{Sample} object containing a timestamp and value. Both values are
stored in primitive \code{double} fields of \code{Sample}.  There is a bit
more code that has to be written, but it is not excessive. Fortunately, the
standard Java \code{Collections} class has some useful static methods so that
new sort and search algorithms do not have to be implemented. The \code{sort}
and \code{binarySearch} methods from \code{Collections} each can take an
\code{ArrayList} and a \code{Comparable} object as parameters. To take
advantage of these methods, the new \code{Sample} class has to implement the
\code{Comparable} interface, so that two sample timestamps can be compared:

\begin{shortlisting}
   
    public class Sample implements Comparable<Sample> {

        private final double timestamp;
        private final double value;
	
        public Sample(double timestamp, double value) {
            this.timestamp = timestamp;
            this.value = value;
        }
	
        public double getTimestamp() {
            return timestamp;
        }
	
        public double getValue() {
            return value;
        }
	      
        public int compareTo(Sample that) {
            return Double.compare(this.getTimestamp(), that.getTimestamp());	
        }
    }

\end{shortlisting}

Additionally, the \code{ArrayList} needs to be stored in a wrapper class that
implements the needed operations. Here are two methods, \code{getValue} and
\code{sort}, of a wrapper class \code{Samples}.

\begin{shortlisting}
    public class Samples {
    
    	public final static double NOT_FOUND = -1d;
        private ArrayList<Sample> samples = 
        	new ArrayList<Sample>();
        ....
		
        public double getValue(double timestamp) {
            Sample sample = new Sample(timestamp, 0.0);
            int result = 
            	Collections.binarySearch(samples, sample);
            if (result < 0) {
                return NOT_FOUND;
            }
            return samples.get(result).getValue();
        }
		
        public void sort() {
            Collections.sort(samples);
            samples.trimToSize();	
        }
    }
    
\end{shortlisting}
  
The EC diagram for 100 entries is shown in Figure~\ref{fig:content-schematic-arraylist-pairs}.
\begin{figure}
  \centering
 \includegraphics[width=0.7\textwidth]{part1/Figures/memoryhealth/arraylist-doubles}
  \caption{EC Diagram for 100 samples stored in an \code{ArrayList} of \code{Samples}}
  \label{fig:content-schematic-arraylist-pairs}
\end{figure} 
This design uses less memory and has better health than the \code{TreeMap}
design. The memory cost is reduced from 6,448 to 2,856 bytes, and the overhead
is reduced from 75\% to 44\%.  There are two reasons for this improvement.
First, the timestamps and values are not boxed. They are stored as primitive
double fields in each \code{Sample}. This reduces the number of objects needed
to store the actual data from 200 to 100. Fewer objects means fewer object
headers and pointers, which is significant in this case. Secondly, an
\code{ArrayList} has lower infrastructure cost than a \code{TreeMap}.
\code{ArrayList} is one of the most memory-efficient Java collections. It is
implemented using a wrapper object and an array of pointers. This is much more
compact than the heavy-weight \code{TreeMap}.

While this is a big improvement, 44\% overhead still seems high. Almost half the
memory is being wasted. How hard is it to get rid of this overhead completely?
Eliminating overhead means eliminating objects altogether. This is not a
recommended practice, unless memory is extremely tight, and there is no other
choice. It is nonetheless an interesting exercise to compare a best case
design with the other designs. The most space-efficient design uses two parallel
arrays of doubles. One array stores all of the sample timestamps, and the second
stores all of the values. These arrays can be stored in a wrapper class that
implements a map interface.

This design requires a bit more code, but, again, it is not excessive. Like the
\code{Collections} class, the \code{Arrays} class provides static \code{sort}
and  \code{binarySearch} methods. However, these methods apply only to a single
array. If you sort the timestamp array, you will lose the association between
the timestamps and their corresponding values. You can use an extra temporary
array to get around this problem. The implementation is left as an exercise. The
EC diagram for this design using two double arrays is shown in
Figure~\ref{fig:content-schematic-arrays-doubles}. The overhead is only 3\%.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{part1/Figures/memoryhealth/array-doubles}
  \caption{EC Diagram for 100 samples stored in two parallel arrays}
  \label{fig:content-schematic-arrays-doubles}
\end{figure}

For the three designs presented, there is a tradeoff between ease of programming
and memory efficiency. More programming is needed as the design becomes more
memory efficient. However, in many cases, memory-efficient designs are just as
easy to implement as inefficient designs. When there are two distinct execution
phases, as in this example, the data structure can be trimmed after the first
phase, to eliminate empty space reserved for growth. Another option is to use
one data structure in the first phase, and copy it into a second data structure
for the second phase. This approach can sometimes be a good compromise between
ease of programming and memory efficiency.
 
\section{Scalability}
\label{sec:scalability}

The evaluation of the three monitoring system designs was based on only 100
samples. In a real scenario, a monitoring system might have to handle hundreds
of thousands, or millions, of samples. Stress testing is usually performed late
in a development cycle, when it is very costly to fix a memory footprint
problem. Using a memory health approach, it possible to predict how well a data
structure design will scale much earlier.
 
\begin{figure}
  \centering
   \includegraphics[width=0.9\textwidth]{part1/Figures/memoryhealth/scalable-health-treemap}
  \caption{Health Measure for the \code{TreeMap} Design Shows Poor Scalability}
  \label{fig:scalable-health-treemap}
\end{figure}
The basic question for predicting scalability is what happens to the bloat
factor as a data structure grows. The \code{TreeMap} design has 75\% overhead
with 100 samples. With 100,000 samples, maybe the high overhead will be
amortized away. Maybe this design will scale well, even if it is inefficient for
small data sizes. The graph in Figure~\ref{fig:scalable-health-treemap}
shows how the \code{TreeMap} design scales as the number of samples increase.
The graph is split into two parts: the percentage of overhead and the percentage
of data. For a single sample, the overhead is 86\%. As more samples are added,
the bloat factor drops to 75\%. Unfortunately, with 200,000 samples, and 300,000
samples, the bloat factor is still 75\%. The \code{TreeMap} design is not only
bloated, but it also does not scale. It is constantly bloated.


%Every collection has \emph{fixed} as well as \emph{variable} overhead
%costs. The fixed overhead is the minimum space needed, no
%matter how many entries are stored in the collection. For \code{TreeMap} it's
%% 48 bytes, the size of the wrapper object. The \emph{variable
%overhead} is the additional space needed to store each entry in
%the collection. In this case it's 32
%bytes, the size of each internal object representing a node in the tree.

%These 32 bytes
%include five pointer fields, in addition to other sources of overhead.

To understand why this happens, recall that the infrastructure of
\code{TreeMap} is a search tree made up of nodes. As samples are added, 
the infrastructure grows, with a new node for every sample. This is
the \emph{variable overhead} of the \code{TreeMap}, the additional
space needed to store an entry. In this case it's 32 bytes,
the cost of an internal node object. Each sample also adds its own overhead,
namely the JVM overhead in the two \code{Double} objects. When the map becomes large enough,
the per-entry overhead --- from the \code{TreeMap} and the
\code{Double}s --- dominates, and hovers around 75\%.

The bloat factor is larger when the \code{TreeMap} is small. That's because for
small \code{TreeMap}s, the \emph{fixed overhead}
is a large component of the cost. A collection's fixed overhead is the base
memory needed, independent of the number of entries stored. For
\code{TreeMap}, it's the 48-byte \code{TreeMap} object.
This fixed cost is quickly amortized as more samples are added.

\callout{callout:fixed-and-per-entry-costs}{Fixed vs.
Variable Collection Overhead}{
\index{Fixed Collection Overhead} \index{Variable Collection Overhead}
Every collection has two types of memory overhead: \textit{fixed}
and \textit{variable}. Together they determine the cost of the collection
for a given number of entries.

Fixed overhead is the minimum space needed, no
matter how many entries are stored in the collection. Small collections with a large fixed
overhead have a high memory bloat factor, but the fixed overhead is amortized
as the collection grows. 

Variable overhead is the memory needed, on average, to
store a new entry in the collection. Collections with a large variable
overhead do not scale well, since the cost of the
collection grows by the variable overhead with each new entry.
}

\begin{figure}
  \centering
   \includegraphics[width=0.9\textwidth]{part1/Figures/memoryhealth/scalable-health-arraylist}
  \caption{Health Measure for the \code{ArrayList} Design }
  \label{fig:scalable-health-arraylist}
\end{figure}

Figure~\ref{fig:scalable-health-arraylist} shows similar data for the
\code{ArrayList} design. Like the \code{TreeMap} design, there is a fixed
overhead which is significant when the \code{ArrayList} is small. As the
\code{ArrayList} grows, the fixed overhead is amortized, but there is
still a per-entry overhead of 43\% that remains constant. That's the combined effect of 
the \code{ArrayList}'s variable overhead and the overhead of each \code{Sample}
object.

For the last design that uses arrays, there is only fixed overhead, namely, the
single \code{Samples} object plus the JVM overhead for the arrays. There is no
additional overhead for each entry, since a scalar
array has no variable overhead, and the samples themselves are pure data.
Figure~\ref{fig:scalable-health-array} shows how the initial 71\% fixed overhead is
quickly amortized. When more samples are added, the bloat
factor becomes 0\%.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{part1/Figures/memoryhealth/scalable-health-array}
  \caption{Health Measure for the Array-Based Design Shows Perfect Scalability}
  \label{fig:scalable-health-array}
\end{figure}

This analysis helps predict the memory requirements for large sample sizes.
Suppose the monitoring system needs to process 10 million samples. The data
alone takes up 160 million bytes (153MB).  The graphs shown in this section can
be used to quickly calculate how much memory you will need. For the
\code{TreeMap} design, the overhead cost is 75\%, so you will need 610MB to
store the samples. For the \code{ArrayList} design, you will need 267MB. For the
\code{array} design, you will need only 153MB. As these numbers show, the design
choice can make a huge difference.

\section{Summary}

When you are developing a large application, closing your eyes to quantitative
considerations and thinking everything is going to be fine is risky. Taking the
time to measure the cost and health of your data design can pinpoint problems
early on. This chapter described three conceptual tools to help evaluate the
memory efficiency of a design.
\begin{itemize}
\item The \textsl{memory bloat factor} measures how much of your the memory
usage of your design is real data, and how much is an artifact of the way you
have chosen to represent it. This metric tells you how much room there is for
improvement, and if in fact you are paying a reasonable price for what you are
getting.
\item Complex data structures are built up from data entities and collections of
these entities.  The \textsl{Entity-Collection (EC) Diagram} shows the costs
associated with the different parts of a complex data structure. These diagrams
help you to compare the memory efficiency of different representation choices.
\item By classifying the overhead of a collection as either \textsl{fixed} or
\textsl{variable}, you can predict how much memory you will need to store very
large collections. Being able to predict scalability is critical to meeting the
requirements of larges applications.
\end{itemize}
These are the basic tools used in the rest of this book. To estimate the cost of
an entity, you will need to know how many bytes each primitive type needs, what
is the pointer size, and what the JVM overhead is. You will learn how to
estimate the cost of entities in \autoref{chapter:delegation}. To estimate scalability,
you will need to know what the fixed and variable costs are for the collection classes
you are using. These are covered in Chapters~\ref{chapter:brief-introduction-collections}
through \ref{chapter:dynamic-records}.

% Nevertheless, you should not throw out all the Java libraries and write
% everything yourself.  It can also prevent unnecessary optimizations that gain
% little or nothing. You should perform thought experiments, use limit cases to
% understand optimal designs, and make back-of-the-envelope calculations.  This
% requires more work, possibly more prototyping, but the effort will likely pays
% off. Subsequent chapters discuss more techniques for precise estimation of
% memory health, and provide the data and overhead costs of common primitives
% and collections needed to perform estimations.
