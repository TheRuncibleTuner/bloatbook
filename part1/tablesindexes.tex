I\chapter{Large Collection Structures}
%\chapter{Large Structures for Accessing Data}
\label{chapter:tables-indexes}

In a relational database system, data is neatly organized for you into tables.
While you may suggest a few fields to index, the structures that
allow you to access your data are taken care of for you.   In object-oriented
programming languages you have more freedom. You have a sea of interconnected
objects, and you are responsible for designing structures are the entry points into various 
groupings of these objects.
In this chapter we look at the memory considerations when
designing large structures for accessing your
data. We'll
look briefly at large collections that just gather data in one place, such as a
list of all the objects of one type. The bulk of the chapter is about
indexes, also known as maps, that let you look up data by value. We'll first look at the costs of large collections,
and at ways to keep them to a minimum for the task at hand. In the second half
of the chapter we'll look at the cost tradeoffs when you need to
design more complex access structures made of multiple levels of collections.  


%Collections, especially large collections, are your
%main tool.
%Maps are the main
%implementation mechanism in these structures \footnote{We use the term map to
%mean a Java collection class, and the term index to describe the general
%functionality.}.

%Large collections, like much else in Java, can
%take up a lot of memory, unless you choose and configure them carefully. 

%, it's easy to create very expensive designs. 


%As we saw in the previous chapter, the
%overhead of different collection choices varies quite a bit. The costs for
%large collections play out somewhat differently than in smaller ones. In
%large collections, whether lists, sets, or maps, the main issue is the
%incremental cost of each new element. This is because the fixed cost of the
%collection is insignificant once the collection is above a certain size.  



%We walk
%through a sample analysis, comparing three alternative designs.


% Here we look at how the costs work out for large collections,
%and what choices are available
%including in open source frameworks if you are able to use these in your
% system.  We'll also look at some
%common special cases. If your requirements fit into one of these cases, there
%are less expensive solutions available. 


%In any large collection, the main
%issue is how to keep down the cost of adding an element. The fixed
%cost of the collection becomes insignificant once the collection is above a
%certain size.  We look at ways to keep these variable costs down: by
%carefully choosing the right collection for the situation; by
%recognizing some special cases that enable some optimizations; and by using
%some of the open source collection classes if they are available to you
%(rephrase all of this is what the reader can do, not as a roadmap).


\section{Large Collections}

Just as with small collections, choosing the right collection for the task
can make a big difference in the memory overhead of large collections.  Suppose
you need to maintain a collection of all the orders processed for the day.  At
the end of the day these orders are posted in bulk to a remote database. The orders
contain their own timestamps, so we don't really care about maintaining the
sequence in which they were received. Figure~\ref{} compares an implementation
using \class{ArrayList} with one using \class{HashSet}. As with small
collections, if we don't need the uniqueness checking or some of the other features of \class{HashSet}, 
then we are clearly much better off with \class{ArrayList}.

The difference in size in the above example
is pretty dramatic, more than 7:1. That reflects the difference in variable
costs of the two collection classes.In larger collections, the variable overhead --- the cost each
element incurs --- determines the cost of the collection. That's because as a
collection grows in size its fixed overhead, such as wrapper objects and array
headers, becomes insignificant. For example, in an \class{ArrayList} with
even 100 elements, the fixed cost is only 9\% of the total. With 1000
elements, it falls to 0.1\%.
%With small collections we need to concern
%ourselves with both fixed and variable costs, for large collections we
%need only look at variable costs. 
As a general rule,
the variable overhead of array-based collections, like \class{ArrayList} is much lower than that of 
entry-based collections where a new entry
object is allocated for each element in the collection. You may have noticed
that most of the collections in the standard collection library are entry-based,
including \class{HashMap}, \class{HashSet}, \class{LinkedList}, and \class{TreeMap}. Table~\ref{} shows a
comparison of variable costs for the most commonly used classes from the standard collections library, 
along with a sampling from open source libraries.


%You may have noticed that in our current example,
%there is a much bigger difference between the two implementations than there
%was in Section~\ref{}, where we compared the same two collection classes in a
%design that had small collections.  For small collections, the fixed costs are
%a larger part of the overhead, whereas here it's only the differences in
%variable costs that matter.  

\paragraph{Excess Capacity}

As we saw in the previous chapter, many collection
classes allocate excess capacity for performance reasons, mainly to accomodate
growth. One difference between small and large collections is the
effect of excess capacity on the overall memory overhead.  

When a collection grows, the amount of spare
capacity allocated is a function of the number of elements in
the collection. For example, \class{ArrayList} reallocates an array that is 50\%
larger than its current size whenever it needs more space, and \class{HashMap}
doubles the number of buckets when the number of elements reaches a
user-specified load factor. So for larger collections, actually any collection
that's grown at least once beyond its initial size, it makes
sense to think of spare capacity as a charge spread over each element of the collection.  In
other words, we can include spare capacity as a component of the variable cost.
If an \class{ArrayList} is 1/3 spare capacity, then we can think of every
element costing 50\% more (i.e. 1/3 free relative to 2/3 used), or 6 bytes
rather than 4.

Collections grow in jumps, rather than on every addition to the
collection, which makes determining the size of a collection a little
more complicated.  For that reason, in Table\ref{} we show
a range of variable costs for each collection.  You can use the minimum
number if there is no spare capacity. The maximum number gives a worst case, if
none of the spare is occupied. Slightly less than the midpoint of
that range will give you an estimate of the variable cost if the most recently
allocated spare capacity is half occupied.

Solutions for reducing excess capacity for larger collections are the same as
for small collections. If you can estimate the number of elements in advance,
you can try to size the collection carefully when you create it.  If your data structure has a
distinct load phase, and the collection has a \code{trimToFit()} call, you can
trim the size of the collection after the load phase is complete. In hash-based collections, such as
\class{HashMap} and \class{HashSet}, excess capacity is needed to reduce the
likelihood of collisions, so it's important to leave headroom for this
purpose. The default load factor is usually a pretty good guide. 

Excess capacity isn't really much of an issue for collections like
\class{HashMap} and \class{HashSet}, where the bulk of the overhead is from the entry objects.
For large array-based collections the excess capacity can be a more significant part of the overhead, 
though it's relative to a more efficient representation in the first place.
Notice that the issue for large collections is not as serious as that for
designs with many very small collections, as we saw
in the previous chapter, where the excess capacity acts as a large fixed cost in each collection.

\paragraph{Entry- vs. Array-based Hash Maps and Hash Sets}

There are two primary ways that hash tables are implemented. Most of the maps and sets
in the standard libraries use a technique known as \emph{chaining} (also
known as open hashing), Figure~\ref{} shows a typical
implementation. There is a separate entry object for element in the collection.
Each entry object points to a key and a value, and possibly contains
additional information such as a cached hash code. There is a linked list of
entry objects for each hash bucket.

The other main technique is known as \emph{open addressing} (for added
confusion, also known as closed hashing).  In this technique,
keys, values, and other information for each element are stored directly in
arrays. These are usually parallel arrays, though some implementations
use a single array and interleave keys, values, and other information in
successive slots. Chains of entries that map to the same bucket are threaded through the arrays.
Figure~\ref{} shows a typical implementation. There are many variations in
practice.

Generally speaking, for larger collections, open addressing hash tables use less
memory --- at least in Java, where the cost of an entry object is so high. The
fastutil and Trove are examples of open source frameworks that provide open
addressing maps and sets that save space. The sets in these frameworks actually
save even more space for a different reason: they are specialized for this
purpose, rather than delegating to a more general map. So unlike in the Java
standard libraries, you are paying only for a value, not a key and value, per entry.

Since open addressing hash tables usually use less memory, why do so many hash
table libraries use chaining? Generally speaking, hash tables based
on chaining are much simpler to implement. More importantly, there
are performance differences between the two approaches, though there is no easy
rule about one always being faster than the other. For your
own system, if performance of your collections is critical it can be worth doing
some timings first.  Keep in mind, that in many systems, the amount of time
spent in hash table lookups is a small fraction of the total execution time to begin with. 

Another thing to be aware of in open addressing implementations is that the
need for excess capacity can reduce the space savings to a greater
extent than in open chaining implementations. So it's important to look at the whole
picture, at the average variable cost, when making decisions on which framework
to use (see Table ~\ref{}).


\paragraph{Sharing the Costs of Entries in Entry-based Collections}

\section{Identity Maps}

\section{Maps with Scalar Keys or Values}

\section{Multikey Maps. Example: Evaluating Three Alternative Designs}

\section{Concurrency and Multilevel Indexes}

\section{Multivalue Maps}

\section{Summary}



