\chapter{Large Structures for Accessing Data}
\label{chapter:tables-indexes}

In a relational database system, data is neatly organized for you into tables.
You may suggest fields to index, and the details of indexes are
taken care of for you. In object-oriented programming languages
you have more freedom. You have a sea of interconnected objects, and you are responsible for designing structures that are the entry points into your
data. Collections, especially large collections, are your
main tool. In this chapter we look at the memory considerations when
designing large collection structures for finding or gathering together your
data. We look briefly
at structures that just collect data in one place, such as a list
of all the objects of one type.Then the bulk of the chapter is about indexes that let you look up data by value. 
Maps are the main implementation
mechanism in these structures \footnote{We use the term map to mean a Java collection class,
and the term index to describe the general functionality.}. 

We start by looking at single large collections, and at ways to
keep their costs down. As we saw with small collections, choosing the minimum
functionality for the task can make a big difference. There are also 
some open source collection classes that can save you a lot of space, if you are
able to use them in your system. We'll next look at a few special cases where
you can use some specialized, less expensive, solutions. The
remainder of the chapter is about analyzing the space costs of more complex
collection structures.  We look at three common cases: an index with a multipart
key, a multilevel concurrent index, and an index with multiple values per key. 
There are more decisions to make when designing these structures, and their costs are
not obvious without a careful analysis. 
%, it's easy to create very expensive designs. 


%As we saw in the previous chapter, the
%overhead of different collection choices varies quite a bit. The costs for
%large collections play out somewhat differently than in smaller ones. In
%large collections, whether lists, sets, or maps, the main issue is the
%incremental cost of each new element. This is because the fixed cost of the
%collection is insignificant once the collection is above a certain size.  



%We walk
%through a sample analysis, comparing three alternative designs.


% Here we look at how the costs work out for large collections,
%and what choices are available
%including in open source frameworks if you are able to use these in your
% system.  We'll also look at some
%common special cases. If your requirements fit into one of these cases, there
%are less expensive solutions available. 


%In any large collection, the main
%issue is how to keep down the cost of adding an element. The fixed
%cost of the collection becomes insignificant once the collection is above a
%certain size.  We look at ways to keep these variable costs down: by
%carefully choosing the right collection for the situation; by
%recognizing some special cases that enable some optimizations; and by using
%some of the open source collection classes if they are available to you
%(rephrase all of this is what the reader can do, not as a roadmap).


\section{Large Collections}

\paragraph{Entry- vs. Array-based Collections}

.. In general, the difference between entry- and
array-based classes is dramatic, as we just saw.  

\paragraph{Excess Capacity}

As we saw in the previous chapter, many collection classes allocate excess
capacity. Sometimes, as in \class{ArrayList}, this is to aid performance
if we are expecting the collection to grow. In hash-based collections, 
such as \class{HashMap} and \class{HashSet}, excess capacity allows for
growth and it also serves to make collisions less likely. In some cases we may
be able to reduce these costs, as we did with
small collections, by sizing the collection carefully when it's created, or if
the data structure has a distinct load phase, by calling \code{trimToFit()} when
the load phase is complete.

Capacity is typically a function of the number of
elements in the collection. (-> verify that it's based on size and not current
capacity) For example, for \class{ArrayList} ..   For \class{HashMap} we specify a load factor. .. details .. 
For large collections, we can think of the spare capacity, as an addition to
the variable cost, the cost each element incurs in the collection. Since
collections grow in jumps, rather than in single steps, it makes sense to look
at this cost on average across many elements. We'll use the term \emph{average
variable cost} of a collection to mean the variable cost of the collection 
plus the average of the excess capacity.  .. examples here ..  include example
of a properly sized ArrayList?

For large array-based collections, since their total overhead is
so much smaller to begin with, the excess capacity can be significant. So from a
relative standpoint, there is more benefit to be found in reducing it.  For
\class{HashMap}, \class{HashSet}, and similar collections, since the bulk of the overhead is from the entry objects, reducing the excess capacity (which is
reflected in the size of the array allocated) will have a much smaller effect on
the size.  In addition, reductions are limited by the need to avoid
collisions, so there's not much point in skimping on capacity in these
structures.


\paragraph{Entry- vs. Array-based Maps and Sets}

There are two primary ways that hash tables are implemented. Most of the maps and sets
in the standard libraries use a technique known as \emph{chaining} (also
known as open hashing).  As we saw in Figure~\ref{}, there is a separate entry
object for each element in the collection.  It points to the key, value, and
possibly additional information such as a cached hash code. There is a linked
list of these entry objects for each hash bucket.

The other main technique is known as \emph{open addressing} (for added
confusion, also known as closed hashing).  In this technique,
keys, values, and other information for each element are stored directly in
arrays. These are usually parallel arrays, though some implementations
use a single array and interleave keys, values, etc. in successive slots. Chains
of entries that map to the same bucket are threaded through the array(s).
Figure~\ref{} shows a typical implementation. There are many variations in
practice.

Generally speaking, for larger collections, open addressing hash tables use less
memory --- at least in Java, where the cost of an object is so high. The fastutil and Trove open source
frameworks provide maps and sets that save space by using open addressing. An added
advantage in both of these frameworks is that their set implementations are specialized for
that purpose, rather than delegating their work to a map. So unlike in the Java standard libraries,
you are paying only for a value, not a key and value, per entry. 

Since open addressing hash tables usually use less memory, why do so many hash
table libraries use chaining? Generally speaking, hash tables based
on chaining are much simpler to implement. More importantly, there
are performance differences between the two approaches, though there is no easy
rule about one always being faster than the other. For your
own system, if performance of your collections is critical it can be worth doing
some timings first.  Keep in mind, though, that in many systems, the amount of time spent in
hash table lookups is a small fraction of the total execution time to begin with. 

Another thing to be aware of in open addressing implementations is that the
need for excess capacity can reduce the space savings to a greater
extent than in open chaining implementations. So it's important to look at the whole
picture, at the average variable cost, when making decisions on which framework
to use (see Table ~\ref{}).


\paragraph{Sharing the Costs of Entries in Entry-based Collections}

\section{Identity Maps}

\section{Maps with Scalar Keys or Values}

\section{Multikey Maps. Example: Evaluating Three Alternative Designs}

\section{Concurrency and Multilevel Indexes}

\section{Multivalue Maps}

\section{Summary}



